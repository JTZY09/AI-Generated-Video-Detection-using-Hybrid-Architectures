{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install torch torchvision opencv-python pillow numpy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neNZiwf6Fvd0",
        "outputId": "34b8b918-12f4-4e36-f910-ac352d96aed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from google.colab import drive\n",
        "import re\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive' , force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJgNZUO-FyF0",
        "outputId": "4eaa6159-c541-4880-ab11-67c041cce552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "real_dir = '/content/drive/MyDrive/data/real_frames'\n",
        "print(f\"Checking directory: {real_dir}\")\n",
        "print(f\"Directory exists: {os.path.exists(real_dir)}\")\n",
        "print(f\"Files in directory: {os.listdir(real_dir)}\")"
      ],
      "metadata": {
        "id": "Ql8HVQ3hIYrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Ensure the mount point is empty before mounting\n",
        "mountpoint = '/content/drive'\n",
        "\n",
        "# Check if the mountpoint exists and is not empty\n",
        "if os.path.exists(mountpoint) and os.path.isdir(mountpoint) and os.listdir(mountpoint):\n",
        "    print(f\"Mountpoint '{mountpoint}' is not empty. Attempting to clear...\")\n",
        "    try:\n",
        "        # Use shell command for recursive removal, typically faster and more robust\n",
        "        !rm -rf {mountpoint}/*\n",
        "        print(f\"Cleared contents of '{mountpoint}'.\")\n",
        "        # Recreate the directory if it was removed by rm -rf *\n",
        "        os.makedirs(mountpoint, exist_ok=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error clearing mountpoint '{mountpoint}': {e}\")\n",
        "        # If clearing fails, you might need manual intervention or skip mounting\n",
        "        # For this case, we'll proceed with mounting, but be aware it might still fail\n",
        "        pass # Or handle the error appropriately\n",
        "\n",
        "# Now mount Google Drive\n",
        "drive.mount(mountpoint , force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZiFF5S45EGr",
        "outputId": "4ed3513d-0380-4ce1-87e9-c5e59938fc30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === The New, Fast, and Space-Efficient Data Setup Script ===\n",
        "# This version copies zips to a temp area and deletes each zip\n",
        "# immediately after unzipping to manage disk space.\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Configuration ---\n",
        "DRIVE_ZIPPED_DATA_DIR = \"/content/drive/MyDrive/zipped\"\n",
        "LOCAL_DATASET_DIR = \"/content/local_data\"\n",
        "# A dedicated temporary directory for the zip files\n",
        "LOCAL_TEMP_ZIP_DIR = \"/content/temp_zips\"\n",
        "# --- End of Configuration ---\n",
        "\n",
        "\n",
        "# --- Main Logic ---\n",
        "print(\"--- Starting Fast & Space-Efficient Data Setup ---\")\n",
        "start_time_total = time.time()\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Define the final local paths for each data type\n",
        "local_real_dir = os.path.join(LOCAL_DATASET_DIR, \"real_frames\")\n",
        "local_fake_dir = os.path.join(LOCAL_DATASET_DIR, \"fake_frames\")\n",
        "local_flow_dir = os.path.join(LOCAL_DATASET_DIR, \"precomputed_flow_features\")\n",
        "\n",
        "# Create the main local data directory and subdirectories\n",
        "os.makedirs(local_real_dir, exist_ok=True)\n",
        "os.makedirs(local_fake_dir, exist_ok=True)\n",
        "os.makedirs(local_flow_dir, exist_ok=True)\n",
        "# NEW: Create the dedicated temporary directory for zip files\n",
        "os.makedirs(LOCAL_TEMP_ZIP_DIR, exist_ok=True)\n",
        "print(f\"Local data directories created at '{LOCAL_DATASET_DIR}'\")\n",
        "\n",
        "\n",
        "# --- Step 1: Copy all zipped chunks into the temporary local directory ---\n",
        "print(f\"\\n--- Step 1: Copying all zip chunks from Drive to '{LOCAL_TEMP_ZIP_DIR}'... ---\")\n",
        "start_time_copy = time.time()\n",
        "\n",
        "source_zip_pattern = os.path.join(DRIVE_ZIPPED_DATA_DIR, \"*.zip\")\n",
        "zip_files_to_copy = glob.glob(source_zip_pattern)\n",
        "\n",
        "if not zip_files_to_copy:\n",
        "    print(f\"❌ ERROR: No .zip files found using glob at '{source_zip_pattern}'. Please re-check the path.\")\n",
        "else:\n",
        "    print(f\"Found {len(zip_files_to_copy)} zip files to copy.\")\n",
        "    for source_path in zip_files_to_copy:\n",
        "        # The destination is now the dedicated temp directory\n",
        "        shutil.copy(source_path, LOCAL_TEMP_ZIP_DIR)\n",
        "\n",
        "    end_time_copy = time.time()\n",
        "    print(f\"-> Copying all zips finished in {end_time_copy - start_time_copy:.2f} seconds.\")\n",
        "\n",
        "\n",
        "# --- Step 2: Unzip, Combine, and Incrementally Delete the Chunks ---\n",
        "print(\"\\n--- Step 2: Unzipping chunks and deleting zips to save space... ---\")\n",
        "\n",
        "# A) Process 'real_frames' chunks\n",
        "# The glob pattern now looks inside the temp directory\n",
        "real_zips = sorted(glob.glob(os.path.join(LOCAL_TEMP_ZIP_DIR, \"real_frames_chunk_*.zip\")))\n",
        "if real_zips:\n",
        "    print(f\"  Unzipping {len(real_zips)} 'real_frames' chunks into '{local_real_dir}'...\")\n",
        "    for zip_file in real_zips:\n",
        "        # Unzip the file\n",
        "        !unzip -q -o \"{zip_file}\" -d \"{local_real_dir}\"\n",
        "        # Immediately delete the zip file to free up space\n",
        "        os.remove(zip_file)\n",
        "        print(f\"    - Unzipped and deleted {os.path.basename(zip_file)}\")\n",
        "else:\n",
        "    print(\"  Warning: No 'real_frames' zip chunks found in temp directory.\")\n",
        "\n",
        "# B) Process 'fake_frames' chunks\n",
        "fake_zips = sorted(glob.glob(os.path.join(LOCAL_TEMP_ZIP_DIR, \"fake_frames_chunk_*.zip\")))\n",
        "if fake_zips:\n",
        "    print(f\"  Unzipping {len(fake_zips)} 'fake_frames' chunks into '{local_fake_dir}'...\")\n",
        "    for zip_file in fake_zips:\n",
        "        !unzip -q -o \"{zip_file}\" -d \"{local_fake_dir}\"\n",
        "        os.remove(zip_file)\n",
        "        print(f\"    - Unzipped and deleted {os.path.basename(zip_file)}\")\n",
        "else:\n",
        "    print(\"  Warning: No 'fake_frames' zip chunks found in temp directory.\")\n",
        "\n",
        "# C) Process 'precomputed_flow' chunk\n",
        "flow_zips = sorted(glob.glob(os.path.join(LOCAL_TEMP_ZIP_DIR, \"Combined_Precomputed_Flow*.zip\")))\n",
        "if flow_zips:\n",
        "    print(f\"  Unzipping {len(flow_zips)} 'precomputed_flow' chunk(s) into '{local_flow_dir}'...\")\n",
        "    for zip_file in flow_zips:\n",
        "        !unzip -q -o \"{zip_file}\" -d \"{local_flow_dir}\"\n",
        "        os.remove(zip_file)\n",
        "        print(f\"    - Unzipped and deleted {os.path.basename(zip_file)}\")\n",
        "else:\n",
        "    print(\"  Warning: No 'precomputed_flow' zips found in temp directory.\")\n",
        "\n",
        "\n",
        "# --- Step 3: Final Verification and Cleanup ---\n",
        "print(\"\\n--- Step 3: Verifying and cleaning up... ---\")\n",
        "try:\n",
        "    num_real = len(os.listdir(local_real_dir))\n",
        "    num_fake = len(os.listdir(local_fake_dir))\n",
        "    num_flow = len(os.listdir(local_flow_dir))\n",
        "\n",
        "    print(f\"  Verification successful:\")\n",
        "    print(f\"  - Real frames folder: {num_real} files\")\n",
        "    print(f\"  - Fake frames folder: {num_fake} files\")\n",
        "    print(f\"  - Flow features folder: {num_flow} files\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"  Verification failed. A directory was not created correctly: {e}\")\n",
        "\n",
        "# Clean up the (now empty) temporary directory for the zips\n",
        "print(f\"  Cleaning up temporary directory: '{LOCAL_TEMP_ZIP_DIR}'...\")\n",
        "shutil.rmtree(LOCAL_TEMP_ZIP_DIR)\n",
        "\n",
        "end_time_total = time.time()\n",
        "print(f\"\\n✅ SUCCESS! Your dataset is ready for training in {end_time_total - start_time_total:.2f} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmSkZP7A2s3E",
        "outputId": "f6640f40-ee5b-4149-c666-5891891be691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Fast & Space-Efficient Data Setup ---\n",
            "Mounted at /content/drive\n",
            "Local data directories created at '/content/local_data'\n",
            "\n",
            "--- Step 1: Copying all zip chunks from Drive to '/content/temp_zips'... ---\n",
            "Found 39 zip files to copy.\n",
            "-> Copying all zips finished in 707.92 seconds.\n",
            "\n",
            "--- Step 2: Unzipping chunks and deleting zips to save space... ---\n",
            "  Unzipping 19 'real_frames' chunks into '/content/local_data/real_frames'...\n",
            "    - Unzipped and deleted real_frames_chunk_01_000000-009999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_02_010000-019999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_03_020000-029999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_04_030000-039999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_05_040000-049999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_06_050000-059999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_07_060000-069999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_08_070000-079999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_09_080000-089999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_10_090000-099999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_11_100000-109999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_12_110000-119999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_13_120000-129999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_14_130000-139999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_15_140000-149999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_16_150000-159999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_17_160000-169999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_18_170000-179999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_19_180000-189999.zip\n",
            "  Unzipping 19 'fake_frames' chunks into '/content/local_data/fake_frames'...\n",
            "    - Unzipped and deleted fake_frames_chunk_01_000000-009999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_02_010000-019999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_03_020000-029999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_04_030000-039999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_05_040000-049999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_06_050000-059999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_07_060000-069999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_08_070000-079999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_09_080000-089999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_10_090000-099999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_11_100000-109999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_12_110000-119999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_13_120000-129999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_14_130000-133999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_15_134000-139899.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_16_139900-149899.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_17_149900-159899.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_18_159900-169899.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_19_169900-170199.zip\n",
            "  Unzipping 1 'precomputed_flow' chunk(s) into '/content/local_data/precomputed_flow_features'...\n",
            "    - Unzipped and deleted Combined_Precomputed_Flow.zip\n",
            "\n",
            "--- Step 3: Verifying and cleaning up... ---\n",
            "  Verification successful:\n",
            "  - Real frames folder: 175840 files\n",
            "  - Fake frames folder: 161107 files\n",
            "  - Flow features folder: 9272 files\n",
            "  Cleaning up temporary directory: '/content/temp_zips'...\n",
            "\n",
            "✅ SUCCESS! Your dataset is ready for training in 1154.94 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "jfwIT2gUAc1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import glob\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import gc\n",
        "import re\n",
        "from PIL import Image\n",
        "import time\n",
        "import math\n",
        "\n",
        "# --- Debug Flag ---\n",
        "DEBUG_VERBOSE = False\n",
        "\n",
        "# --- Global Configurations ---\n",
        "# UPDATED to match your new data preprocessing\n",
        "SEQUENCE_LENGTH = 30\n",
        "FLOW_DIM = 2\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "# --- PATHS (ASSUMING DATA IS ALREADY COPIED LOCALLY) ---\n",
        "# INPUT: Pointing to your new directories with 60 frames per video\n",
        "LOCAL_REAL_FRAMES_DIR = '/content/local_data/real_frames'\n",
        "LOCAL_FAKE_FRAMES_DIR = '/content/local_data/fake_frames'\n",
        "# INPUT: Pointing to your new flow directory for 30-frame sequences\n",
        "PRECOMPUTED_FLOW_DIR_FOR_DATASET = '/content/local_data/precomputed_flow_features'\n",
        "\n",
        "# --- Output Paths (on Google Drive) ---\n",
        "DRIVE_OUTPUT_BASE = f'/content/drive/MyDrive/final_balanced_data/cnn_lstm{SEQUENCE_LENGTH}_output'\n",
        "LOG_FILE_PATH = os.path.join(DRIVE_OUTPUT_BASE, 'training_log.txt')\n",
        "BEST_MODEL_SAVE_PATH = os.path.join(DRIVE_OUTPUT_BASE, f'best_model.pth')\n",
        "OUTPUT_DIR_FOR_PLOTS_AND_REPORTS = os.path.join(DRIVE_OUTPUT_BASE, 'reports_and_plots')\n",
        "LATEST_CHECKPOINT_PATH = os.path.join(DRIVE_OUTPUT_BASE, 'latest_checkpoint.pth')\n",
        "\n",
        "\n",
        "# Fallback local paths\n",
        "LOCAL_OUTPUT_BASE_FALLBACK = f'/content/outputs_local_cnn_lstm_SL{SEQUENCE_LENGTH}'\n",
        "_LOG_FILE_PATH_TEMP = os.path.join(LOCAL_OUTPUT_BASE_FALLBACK, 'training_log.txt')\n",
        "_BEST_MODEL_SAVE_PATH_TEMP = os.path.join(LOCAL_OUTPUT_BASE_FALLBACK, f'best_model.pth')\n",
        "_OUTPUT_DIR_FOR_PLOTS_AND_REPORTS_TEMP = os.path.join(LOCAL_OUTPUT_BASE_FALLBACK, 'reports_and_plots')\n",
        "\n",
        "# --- Mount Drive and Setup Output Dirs ---\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Google Drive mounted for saving outputs.\")\n",
        "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(BEST_MODEL_SAVE_PATH), exist_ok=True)\n",
        "    os.makedirs(OUTPUT_DIR_FOR_PLOTS_AND_REPORTS, exist_ok=True)\n",
        "    print(f\"Output directories on Drive ensured/created at {DRIVE_OUTPUT_BASE}\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Drive mount/setup error: {e}. Using local fallback for outputs.\")\n",
        "    DRIVE_OUTPUT_BASE = LOCAL_OUTPUT_BASE_FALLBACK\n",
        "    LOG_FILE_PATH = _LOG_FILE_PATH_TEMP; BEST_MODEL_SAVE_PATH = _BEST_MODEL_SAVE_PATH_TEMP\n",
        "    OUTPUT_DIR_FOR_PLOTS_AND_REPORTS = _OUTPUT_DIR_FOR_PLOTS_AND_REPORTS_TEMP\n",
        "    for p_dir in [os.path.dirname(LOG_FILE_PATH), os.path.dirname(BEST_MODEL_SAVE_PATH), OUTPUT_DIR_FOR_PLOTS_AND_REPORTS]:\n",
        "        os.makedirs(p_dir, exist_ok=True)\n",
        "    print(f\"Using LOCAL fallback paths for outputs: {DRIVE_OUTPUT_BASE}\")\n",
        "\n",
        "# --- DeepfakeFrameDataset Class (Corrected for Symmetric Flow Loading) ---\n",
        "class DeepfakeFrameDataset(Dataset):\n",
        "    def __init__(self, sequences_info_list, sequence_length=SEQUENCE_LENGTH, transform=None):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.transform = transform\n",
        "        self.sequences_info = [info for info in sequences_info_list if info is not None and info[0] is not None]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self.sequences_info):\n",
        "            return None, None, None\n",
        "        seq_paths, label, sequence_identifier = self.sequences_info[idx]\n",
        "        if not isinstance(seq_paths, list) or len(seq_paths) != self.sequence_length:\n",
        "            return None, None, None\n",
        "\n",
        "        # --- Frame loading ---\n",
        "        frames = []\n",
        "        for path_str in seq_paths:\n",
        "            try:\n",
        "                img = Image.open(str(path_str)).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                frames.append(img)\n",
        "            except:\n",
        "                return None, None, None\n",
        "        if len(frames) != self.sequence_length:\n",
        "            return None, None, None\n",
        "        frames_tensor = torch.stack(frames)\n",
        "\n",
        "        # --- Flow loading ---\n",
        "        flow_feature_filename = f\"{sequence_identifier}_flow.pt\"\n",
        "        flow_feature_path = os.path.join(PRECOMPUTED_FLOW_DIR_FOR_DATASET, flow_feature_filename)\n",
        "\n",
        "        flow_features_sequence = None\n",
        "        try:\n",
        "            loaded_flow = torch.load(flow_feature_path)\n",
        "            if loaded_flow.shape == (self.sequence_length, 2):\n",
        "                flow_features_sequence = loaded_flow\n",
        "        except:\n",
        "            flow_features_sequence = None  # <--- now returns None if missing\n",
        "\n",
        "        return frames_tensor, flow_features_sequence, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# --- DeepfakeDetectionModel Class ---\n",
        "class DeepfakeDetectionModel(nn.Module):\n",
        "    def __init__(self, cnn_output_dim=512, lstm_hidden_dim=128, lstm_layers=1,\n",
        "                 flow_dim=FLOW_DIM, num_classes=NUM_CLASSES, dropout_rate=0.5, freeze_cnn_layers=False):\n",
        "        super(DeepfakeDetectionModel, self).__init__()\n",
        "        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        self.cnn_backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        if freeze_cnn_layers:\n",
        "            layers_to_freeze_idx = 7\n",
        "            for i, child in enumerate(self.cnn_backbone.children()):\n",
        "                if i < layers_to_freeze_idx:\n",
        "                    for param in child.parameters():\n",
        "                        param.requires_grad = False\n",
        "\n",
        "        self.cnn_output_dim = resnet.fc.in_features\n",
        "        self.lstm = nn.LSTM(input_size=self.cnn_output_dim,\n",
        "                            hidden_size=lstm_hidden_dim,\n",
        "                            num_layers=lstm_layers,\n",
        "                            batch_first=True,\n",
        "                            dropout=dropout_rate if lstm_layers > 1 else 0)\n",
        "\n",
        "        # new feature dimension = 2 * hidden_dim (mean + max pooling)\n",
        "        pooled_dim = 2 * lstm_hidden_dim\n",
        "\n",
        "        self.classifier_with_flow = nn.Sequential(\n",
        "            nn.Linear(pooled_dim + flow_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "        self.classifier_no_flow = nn.Sequential(\n",
        "            nn.Linear(pooled_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, frames, flow_features_sequence):\n",
        "        batch_size, seq_len, _, _, _ = frames.size()\n",
        "        cnn_features_list = []\n",
        "        for t in range(seq_len):\n",
        "            frame_input = frames[:, t, :, :, :]\n",
        "            feat = self.cnn_backbone(frame_input)\n",
        "            feat = feat.view(batch_size, -1)\n",
        "            cnn_features_list.append(feat)\n",
        "        cnn_sequence_features = torch.stack(cnn_features_list, dim=1)\n",
        "\n",
        "        lstm_out, _ = self.lstm(cnn_sequence_features)\n",
        "        lstm_mean = torch.mean(lstm_out, dim=1)\n",
        "        lstm_max = torch.max(lstm_out, dim=1).values\n",
        "        lstm_features = torch.cat((lstm_mean, lstm_max), dim=1)\n",
        "\n",
        "        if flow_features_sequence is not None and \\\n",
        "           flow_features_sequence.ndim == 3 and \\\n",
        "           flow_features_sequence.shape[1] == seq_len:\n",
        "            aggregated_flow_features = torch.mean(flow_features_sequence, dim=1)\n",
        "            combined_features = torch.cat((lstm_features, aggregated_flow_features), dim=1)\n",
        "            output = self.classifier_with_flow(combined_features)\n",
        "        else:\n",
        "            output = self.classifier_no_flow(lstm_features)\n",
        "        return output\n",
        "\n",
        "\n",
        "# --- Training Function ---\n",
        "def train_model(model, train_loader, val_loader, device, sequence_length,\n",
        "                num_epochs=10, accum_steps=2, learning_rate=0.0005, weight_decay=1e-5,\n",
        "                resume_from_checkpoint=True):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    cnn_params = [p for p in model.cnn_backbone.parameters() if p.requires_grad]\n",
        "    other_params = [p for n, p in model.named_parameters() if \"cnn_backbone.\" not in n and p.requires_grad]\n",
        "    optimizer = optim.AdamW([\n",
        "        {'params': cnn_params, 'lr': learning_rate / 10},\n",
        "        {'params': other_params, 'lr': learning_rate}\n",
        "    ], weight_decay=weight_decay)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=3)\n",
        "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'epoch_time': []}\n",
        "    best_val_loss = float('inf'); epochs_no_improve = 0; patience_early_stop = 3\n",
        "    start_epoch = 0\n",
        "\n",
        "    # --- Resume from checkpoint if available ---\n",
        "    if resume_from_checkpoint and os.path.exists(LATEST_CHECKPOINT_PATH):\n",
        "        print(f\"Resuming training from checkpoint: {LATEST_CHECKPOINT_PATH}\")\n",
        "        checkpoint = torch.load(LATEST_CHECKPOINT_PATH, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        best_val_loss = checkpoint['best_val_loss']\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        history = checkpoint['history']\n",
        "        print(f\"Resumed from epoch {start_epoch}, best val loss so far: {best_val_loss:.4f}\")\n",
        "\n",
        "    print(f\"--- Training CNN-LSTM for {num_epochs} epochs ---\")\n",
        "    lr_info = [f\"G{i}_LR:{pg['lr']:.1e}({len(pg['params'])}p)\" for i, pg in enumerate(optimizer.param_groups)]\n",
        "    print(f\"Optimizer: AdamW, {', '.join(lr_info)}, WD:{weight_decay}, ES:{patience_early_stop}\")\n",
        "\n",
        "    total_start_time = time.time()\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train(); train_loss_sum = 0.0\n",
        "        train_preds_e, train_labels_e = [], []\n",
        "        optimizer.zero_grad(); proc_batches = 0\n",
        "\n",
        "        # --- Training loop ---\n",
        "        for i, batch_data in enumerate(train_loader):\n",
        "            if batch_data is None or batch_data[0] is None: continue\n",
        "            frames, flow, labels = batch_data\n",
        "            if frames is None or flow is None or labels is None: continue\n",
        "            frames, flow, labels = frames.to(device), flow.to(device), labels.to(device)\n",
        "            outputs = model(frames, flow)\n",
        "            loss = criterion(outputs, labels)\n",
        "            (loss / accum_steps).backward()\n",
        "\n",
        "            if (i + 1) % accum_steps == 0 or (i + 1) == len(train_loader):\n",
        "                optimizer.step(); optimizer.zero_grad()\n",
        "\n",
        "            train_loss_sum += loss.item()\n",
        "            _, preds_ = torch.max(outputs, 1)\n",
        "            train_preds_e.extend(preds_.cpu().numpy()); train_labels_e.extend(labels.cpu().numpy())\n",
        "            proc_batches += 1\n",
        "            del frames, flow, labels, outputs, loss\n",
        "\n",
        "        avg_train_loss = train_loss_sum / proc_batches if proc_batches > 0 else float('inf')\n",
        "        train_acc = accuracy_score(train_labels_e, train_preds_e) if train_labels_e else 0.0\n",
        "        history['train_loss'].append(avg_train_loss); history['train_acc'].append(train_acc)\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval(); val_loss_sum = 0.0; val_preds_e, val_labels_e = [], []\n",
        "        proc_val_batches = 0\n",
        "        if val_loader:\n",
        "            with torch.no_grad():\n",
        "                for batch_data_val in val_loader:\n",
        "                    if batch_data_val is None or batch_data_val[0] is None: continue\n",
        "                    frames, flow, labels = batch_data_val\n",
        "                    if frames is None or flow is None or labels is None: continue\n",
        "                    frames, flow, labels = frames.to(device), flow.to(device), labels.to(device)\n",
        "                    outputs = model(frames, flow)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss_sum += loss.item()\n",
        "                    _, preds_ = torch.max(outputs, 1)\n",
        "                    val_preds_e.extend(preds_.cpu().numpy()); val_labels_e.extend(labels.cpu().numpy())\n",
        "                    proc_val_batches += 1\n",
        "\n",
        "        avg_val_loss = val_loss_sum / proc_val_batches if proc_val_batches > 0 else float('inf')\n",
        "        val_acc = accuracy_score(val_labels_e, val_preds_e) if val_labels_e else 0.0\n",
        "        history['val_loss'].append(avg_val_loss); history['val_acc'].append(val_acc)\n",
        "\n",
        "        epoch_dur = time.time() - epoch_start_time\n",
        "        history['epoch_time'].append(epoch_dur)\n",
        "        lrs_log = [f\"{g['lr']:.1e}\" for g in optimizer.param_groups]\n",
        "        log_msg = (f\"E{epoch+1}/{num_epochs}, TrL:{avg_train_loss:.4f}, TrAcc:{train_acc:.4f}, \"\n",
        "                   f\"VL:{avg_val_loss:.4f}, VAcc:{val_acc:.4f}, Dur:{epoch_dur:.2f}s, LRs:{lrs_log}\")\n",
        "        print(log_msg)\n",
        "        with open(LOG_FILE_PATH, 'a') as f: f.write(log_msg + '\\n')\n",
        "\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # --- Save best model ---\n",
        "        if proc_val_batches > 0 and avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), BEST_MODEL_SAVE_PATH)\n",
        "            print(f\"  Best model saved: VL={best_val_loss:.4f}, VAcc={val_acc:.4f}\")\n",
        "            epochs_no_improve = 0\n",
        "        elif proc_val_batches > 0:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        # --- Save latest checkpoint (always) ---\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'best_val_loss': best_val_loss,\n",
        "            'history': history\n",
        "        }, LATEST_CHECKPOINT_PATH)\n",
        "\n",
        "        if epochs_no_improve >= patience_early_stop:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "    total_dur = time.time() - total_start_time\n",
        "    print(f\"Training done. Total: {total_dur:.2f}s ({total_dur/60:.2f}m)\")\n",
        "    with open(LOG_FILE_PATH, 'a') as f: f.write(f\"\\nTotal training time: {total_dur:.2f}s\\n\")\n",
        "    return model, history\n",
        "\n",
        "\n",
        "# --- Other Helper Functions (Plotting, Evaluation, Grouping, Collate) ---\n",
        "def plot_training_history(history, save_dir):\n",
        "    if not history or not history.get('train_loss') : print(\"Warn: History incomplete.\"); return\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
        "    color = 'tab:red'; ax1.set_xlabel('Epochs'); ax1.set_ylabel('Loss', color=color)\n",
        "    ax1.plot(epochs, history['train_loss'], 'bo-', label='Train Loss')\n",
        "    if 'val_loss' in history and history['val_loss'] and any(v!=float('inf') for v in history['val_loss']):\n",
        "        ax1.plot(epochs, history['val_loss'], 'ro-', label='Val Loss')\n",
        "    ax1.tick_params(axis='y', labelcolor=color); ax1.legend(loc='upper left'); ax1.grid(True, linestyle=':')\n",
        "    ax2 = ax1.twinx(); color = 'tab:blue'; ax2.set_ylabel('Accuracy', color=color)\n",
        "    if 'train_acc' in history and history['train_acc']: ax2.plot(epochs, history['train_acc'], 'bs--', label='Train Acc')\n",
        "    if 'val_acc' in history and history['val_acc']: ax2.plot(epochs, history['val_acc'], 'rs--', label='Val Acc')\n",
        "    ax2.tick_params(axis='y', labelcolor=color); ax2.legend(loc='upper right')\n",
        "    fig.tight_layout(); plt.title('CNN-LSTM Training History')\n",
        "    plt.savefig(os.path.join(save_dir, 'training_history_cnn_lstm.png')); plt.close()\n",
        "    print(f\"Plot saved to {os.path.join(save_dir, 'training_history_cnn_lstm.png')}\")\n",
        "\n",
        "def evaluate_model_and_report(model, test_loader, device, sequence_length, output_dir):\n",
        "    model.eval(); all_lbls, all_preds_ = [], []\n",
        "    print(\"\\n--- Evaluation (CNN-LSTM) ---\")\n",
        "    with torch.no_grad():\n",
        "        for batch_data in test_loader:\n",
        "            if batch_data is None or batch_data[0] is None: continue\n",
        "            frames, flow, labels = batch_data\n",
        "            if frames is None : continue\n",
        "            frames, flow, labels = frames.to(device), flow.to(device), labels.to(device)\n",
        "            outputs = model(frames, flow); _, preds = torch.max(outputs, 1)\n",
        "            all_lbls.extend(labels.cpu().numpy()); all_preds_.extend(preds.cpu().numpy())\n",
        "    if not all_lbls: print(\"Eval failed: no labels.\"); return\n",
        "    acc = accuracy_score(all_lbls, all_preds_); f1 = f1_score(all_lbls, all_preds_, average='weighted', zero_division=0)\n",
        "    report = classification_report(all_lbls, all_preds_, target_names=['Real','Fake'], zero_division=0)\n",
        "    cm = confusion_matrix(all_lbls, all_preds_)\n",
        "    print(f\"Accuracy: {acc:.4f}, Weighted F1: {f1:.4f}\\n{report}\\nCM:\\n{cm}\")\n",
        "    report_path = os.path.join(output_dir, 'eval_report_cnn_lstm.txt')\n",
        "    with open(report_path, 'w') as f: f.write(f\"Acc:{acc:.4f}\\nF1:{f1:.4f}\\n{report}\\nCM:\\n{cm}\")\n",
        "    print(f\"Eval report saved to {report_path}\")\n",
        "    plt.figure(figsize=(4,3)); sns.heatmap(cm,annot=True,fmt='d',cmap='Blues',xticklabels=['R','F'],yticklabels=['R','F']);plt.title('CM');\n",
        "    plt.savefig(os.path.join(output_dir, 'cm_cnn_lstm.png')); plt.close()\n",
        "\n",
        "# --- Group Frames By Video Name---\n",
        "def group_frames_by_video(frame_paths):\n",
        "    video_groups = defaultdict(list)\n",
        "    for path_str in frame_paths:\n",
        "        stem = Path(str(path_str)).stem\n",
        "\n",
        "        # --- THIS IS THE KEY FIX ---\n",
        "        # This regex looks for a base name, followed by an optional '_frame_' or '_f_', and ends with numbers.\n",
        "        # It's designed to capture the \"video stem\" part robustly.\n",
        "        match = re.match(r'^(.*?)_([frame_f_]+)?(\\d+)$', stem)\n",
        "\n",
        "        if match:\n",
        "            # The video stem is the first captured group\n",
        "            video_stem = match.group(1)\n",
        "        else:\n",
        "            # If the regex fails, we have a fallback, but this should be rare.\n",
        "            video_stem = stem\n",
        "        # ---------------------------\n",
        "\n",
        "        video_groups[video_stem].append(str(path_str))\n",
        "\n",
        "    for stem_key in list(video_groups.keys()):\n",
        "        try:\n",
        "            # A more robust way to sort by the final number in the filename\n",
        "            video_groups[stem_key] = sorted(\n",
        "                video_groups[stem_key],\n",
        "                key=lambda x: int(re.search(r'(\\d+)$', Path(x).stem).group(1))\n",
        "            )\n",
        "        except (AttributeError, ValueError):\n",
        "            video_groups[stem_key] = sorted(video_groups[stem_key])\n",
        "\n",
        "    return video_groups\n",
        "\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    batch = [item for item in batch if item and all(t is not None for t in item) and \\\n",
        "             item[0].shape[0]==SEQUENCE_LENGTH and item[1].shape[0]==SEQUENCE_LENGTH]\n",
        "    return torch.utils.data.dataloader.default_collate(batch) if batch else (None,None,None)\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == '__main__':\n",
        "    print(\"### This script ASSUMES data (frames & flow) is ALREADY copied to local Colab storage. ###\")\n",
        "    print(f\"Expected REAL frames at: {LOCAL_REAL_FRAMES_DIR}\")\n",
        "    print(f\"Expected FAKE frames at: {LOCAL_FAKE_FRAMES_DIR}\")\n",
        "    print(f\"Expected FLOW features at: {PRECOMPUTED_FLOW_DIR_FOR_DATASET}\")\n",
        "\n",
        "    print(\"--- Main CNN-LSTM Training Script (with 30-frame sequences) ---\")\n",
        "    batch_size = 8\n",
        "    accum_steps = 2           # effective batch size 16\n",
        "    num_epochs = 10\n",
        "    learning_rate_param = 5e-6\n",
        "    weight_decay_param = 5e-4\n",
        "    dropout_rate_param = 0.5\n",
        "    freeze_cnn_layers_initially = True\n",
        "\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"--- Config: Device={device}, SeqLen={SEQUENCE_LENGTH}, Batch={batch_size}, Epochs={num_epochs}, LR={learning_rate_param}, WD={weight_decay_param}, Dropout={dropout_rate_param}\")\n",
        "    print(f\"Data Dirs (Local): Real='{LOCAL_REAL_FRAMES_DIR}', Fake='{LOCAL_FAKE_FRAMES_DIR}', Flow='{PRECOMPUTED_FLOW_DIR_FOR_DATASET}'\")\n",
        "    print(f\"Freeze CNN: {freeze_cnn_layers_initially}\")\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "    # Mild crop to introduce variation, but keep most of the face region\n",
        "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n",
        "    # Horizontal flip makes sense (faces left/right)\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    # Light color jitter (avoid washing out fake texture artifacts)\n",
        "    transforms.ColorJitter(\n",
        "        brightness=0.1,\n",
        "        contrast=0.1,\n",
        "        saturation=0.05,\n",
        "        hue=0.02\n",
        "    ),\n",
        "    # Keep orientation stable (rotation can break flow cues)\n",
        "    # transforms.RandomRotation(15)  # ❌ Remove, breaks temporal consistency\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "    # Small chance of erasing, but much gentler\n",
        "    transforms.RandomErasing(\n",
        "        p=0.1,\n",
        "        scale=(0.02, 0.1),\n",
        "        ratio=(0.5, 2.0),\n",
        "        value=0\n",
        "    )\n",
        "    ])\n",
        "    # Validation/Test: no augmentation, just normalization\n",
        "    val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    # Keep the validation/test transform simple for consistent evaluation\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    for pth, name in [(LOCAL_REAL_FRAMES_DIR, \"Real frames\"), (LOCAL_FAKE_FRAMES_DIR, \"Fake frames\"), (PRECOMPUTED_FLOW_DIR_FOR_DATASET, \"Flow features\")]:\n",
        "        if not (os.path.exists(pth) and os.path.isdir(pth)):\n",
        "            print(f\"CRITICAL: {name} directory {pth} missing locally. Exiting.\"); exit()\n",
        "        if not os.listdir(pth) and pth != PRECOMPUTED_FLOW_DIR_FOR_DATASET:\n",
        "            print(f\"CRITICAL: {name} directory {pth} is empty locally. Exiting.\"); exit()\n",
        "        elif pth == PRECOMPUTED_FLOW_DIR_FOR_DATASET and not os.listdir(pth):\n",
        "             print(f\"Warning: Local flow directory {PRECOMPUTED_FLOW_DIR_FOR_DATASET} is empty. Flow features will default to zeros.\")\n",
        "\n",
        "    print(f\"\\n--- Loading Frame Paths from local storage ---\")\n",
        "    img_patterns = [\"*.[jJ][pP][gG]\", \"*.[jJ][pP][eE][gG]\", \"*.[pP][nN][gG]\"]\n",
        "    all_real_frame_paths = sorted(list(set(p for pattern in img_patterns for p in glob.glob(os.path.join(LOCAL_REAL_FRAMES_DIR, '**', pattern), recursive=True))))\n",
        "    all_fake_frame_paths = sorted(list(set(p for pattern in img_patterns for p in glob.glob(os.path.join(LOCAL_FAKE_FRAMES_DIR, '**', pattern), recursive=True))))\n",
        "    print(f\"Frames: Real={len(all_real_frame_paths)}, Fake={len(all_fake_frame_paths)}\")\n",
        "    if not all_real_frame_paths and not all_fake_frame_paths: raise ValueError(\"No frames loaded.\")\n",
        "\n",
        "    print(\"\\n--- Grouping Frames and Creating Sequences ---\")\n",
        "    real_video_groups = group_frames_by_video(all_real_frame_paths)\n",
        "    fake_video_groups = group_frames_by_video(all_fake_frame_paths)\n",
        "    print(f\"Groups: Real={len(real_video_groups)}, Fake={len(fake_video_groups)}\")\n",
        "\n",
        "    all_sequences_info = []\n",
        "    STEP_TRAIN_SEQ = 30 # Must match STEP_SIZE from flow preprocessing\n",
        "    for lbl_str, groups in [(\"real\", real_video_groups), (\"fake\", fake_video_groups)]:\n",
        "        lbl = 0 if lbl_str == \"real\" else 1\n",
        "        for video_stem, frames_list in groups.items():\n",
        "            if len(frames_list) < SEQUENCE_LENGTH: continue\n",
        "            for i in range(0, len(frames_list) - SEQUENCE_LENGTH + 1, STEP_TRAIN_SEQ):\n",
        "                seq_paths = frames_list[i : i + SEQUENCE_LENGTH]\n",
        "                if len(seq_paths) == SEQUENCE_LENGTH:\n",
        "                    safe_stem = re.sub(r'[\\\\/*?:\"<>|]',\"_\", str(video_stem))\n",
        "                    seq_id = f\"{lbl_str}_{safe_stem}_seq{i//STEP_TRAIN_SEQ}\"\n",
        "                    all_sequences_info.append((seq_paths, lbl, seq_id))\n",
        "    num_r, num_f = sum(1 for _,l,_ in all_sequences_info if l==0), sum(1 for _,l,_ in all_sequences_info if l==1)\n",
        "    print(f\"Total sequences created: {len(all_sequences_info)} (R:{num_r}, F:{num_f})\")\n",
        "    if not all_sequences_info: raise ValueError(\"No sequences created.\")\n",
        "\n",
        "    print(\"\\n--- Balancing & Splitting Data ---\")\n",
        "    if num_r > 0 and num_f > 0:\n",
        "        min_c = min(num_r, num_f); print(f\"Balancing to {min_c} per class.\")\n",
        "        real_s = random.sample([s for s in all_sequences_info if s[1]==0], min_c)\n",
        "        fake_s = random.sample([s for s in all_sequences_info if s[1]==1], min_c)\n",
        "        balanced_sequences_info = real_s + fake_s; random.shuffle(balanced_sequences_info)\n",
        "    else: balanced_sequences_info = all_sequences_info; print(\"Using imbalanced data.\")\n",
        "    if not balanced_sequences_info: raise ValueError(\"No sequences after balancing.\")\n",
        "    print(f\"Balanced sequences: {len(balanced_sequences_info)}\")\n",
        "\n",
        "    labels_strat = [info[1] for info in balanced_sequences_info]\n",
        "    # Simplified 80/10/10 split\n",
        "    train_val_info, test_dataset_info = train_test_split(balanced_sequences_info, test_size=0.1, stratify=labels_strat if len(np.unique(labels_strat)) > 1 else None, random_state=42)\n",
        "    train_labels_strat = [info[1] for info in train_val_info] if train_val_info else []\n",
        "    if len(train_val_info) < 2 or len(np.unique(train_labels_strat)) < 2:\n",
        "        train_dataset_info=train_val_info; val_dataset_info=[]\n",
        "    else: train_dataset_info, val_dataset_info = train_test_split(train_val_info, test_size=0.111111, stratify=train_labels_strat, random_state=42)\n",
        "    print(f\"Split sizes: Train={len(train_dataset_info)}, Val={len(val_dataset_info)}, Test={len(test_dataset_info)}\")\n",
        "    if not train_dataset_info: raise ValueError(\"Train dataset info is empty.\")\n",
        "\n",
        "    # --- NEW, CORRECTED LINES ---\n",
        "    train_dataset = DeepfakeFrameDataset(train_dataset_info, transform=train_transform)\n",
        "    # The validation and test sets use the simple, consistent transform\n",
        "    val_dataset = DeepfakeFrameDataset(val_dataset_info, transform=val_test_transform) if val_dataset_info else None\n",
        "    test_dataset = DeepfakeFrameDataset(test_dataset_info, transform=val_test_transform) if test_dataset_info else None\n",
        "\n",
        "    num_workers_dl = 2 if device.type == 'cuda' else 0; pin_mem = num_workers_dl > 0\n",
        "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=num_workers_dl, collate_fn=custom_collate_fn, pin_memory=pin_mem, prefetch_factor=2 if pin_mem and num_workers_dl > 0 else None)\n",
        "    val_loader = DataLoader(val_dataset, batch_size, shuffle=False, num_workers=num_workers_dl, collate_fn=custom_collate_fn, pin_memory=pin_mem, prefetch_factor=2 if pin_mem and num_workers_dl > 0 else None) if val_dataset and len(val_dataset)>0 else None\n",
        "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=num_workers_dl, collate_fn=custom_collate_fn, pin_memory=pin_mem, prefetch_factor=2 if pin_mem and num_workers_dl > 0 else None) if test_dataset and len(test_dataset)>0 else None\n",
        "    print(\"DataLoaders ready.\")\n",
        "\n",
        "    print(\"\\n--- Initializing CNN-LSTM Model ---\")\n",
        "    model = DeepfakeDetectionModel(\n",
        "        lstm_hidden_dim=128, # Example, adjust as needed\n",
        "        lstm_layers=1,\n",
        "        dropout_rate=dropout_rate_param,\n",
        "        freeze_cnn_layers=freeze_cnn_layers_initially\n",
        "    ).to(device)\n",
        "\n",
        "    if train_loader and len(train_dataset) > 0:\n",
        "        print(\"\\n--- Starting CNN-LSTM Model Training ---\")\n",
        "        with open(LOG_FILE_PATH, 'w') as f: f.write(f\"--- Training Log (CNN-LSTM, SL{SEQUENCE_LENGTH}) ---\\n\")\n",
        "        model, training_history = train_model(\n",
        "            model, train_loader, val_loader, device, SEQUENCE_LENGTH, num_epochs, accum_steps,\n",
        "            learning_rate_param, weight_decay_param\n",
        "        )\n",
        "    else: print(\"Skipping training: Train loader or dataset invalid.\")\n",
        "\n",
        "    if training_history and training_history.get('train_loss') and any(v!=float('inf') for v in training_history['train_loss']):\n",
        "        plot_training_history(training_history, OUTPUT_DIR_FOR_PLOTS_AND_REPORTS)\n",
        "\n",
        "    model_path_to_load_for_eval = BEST_MODEL_SAVE_PATH\n",
        "    if os.path.exists(model_path_to_load_for_eval):\n",
        "        print(f\"\\n--- Evaluation (Best CNN-LSTM Model from: {model_path_to_load_for_eval}) ---\")\n",
        "        final_model = DeepfakeDetectionModel(\n",
        "             dropout_rate=dropout_rate_param,\n",
        "             freeze_cnn_layers=False # Eval mode\n",
        "        ).to(device)\n",
        "        try:\n",
        "            final_model.load_state_dict(torch.load(model_path_to_load_for_eval, map_location=device))\n",
        "            print(\"Best model loaded for evaluation.\")\n",
        "            if test_loader and test_dataset and len(test_dataset)>0: evaluate_model_and_report(final_model, test_loader, device, SEQUENCE_LENGTH, OUTPUT_DIR_FOR_PLOTS_AND_REPORTS)\n",
        "        except Exception as e: print(f\"Error loading best model: {e}\")\n",
        "    elif 'model' in locals() and model is not None and training_history and training_history.get('train_loss') and any(v!=float('inf') for v in training_history['train_loss']):\n",
        "        print(f\"--- Evaluation (Current CNN-LSTM Model from training) ---\")\n",
        "        if test_loader and test_dataset and len(test_dataset)>0: evaluate_model_and_report(model, test_loader, device, SEQUENCE_LENGTH, OUTPUT_DIR_FOR_PLOTS_AND_REPORTS)\n",
        "    else: print(\"No model available for evaluation.\")\n",
        "\n",
        "    gc.collect();\n",
        "    if device.type == 'cuda': torch.cuda.empty_cache()\n",
        "    print(\"\\n--- Main CNN-LSTM script execution finished ---\")"
      ],
      "metadata": {
        "id": "fZSa87hmP96C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f69ea99c-0808-45b8-94e3-59648abeaf0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted for saving outputs.\n",
            "Output directories on Drive ensured/created at /content/drive/MyDrive/final_balanced_data/cnn_lstm30_output\n",
            "### This script ASSUMES data (frames & flow) is ALREADY copied to local Colab storage. ###\n",
            "Expected REAL frames at: /content/local_data/real_frames\n",
            "Expected FAKE frames at: /content/local_data/fake_frames\n",
            "Expected FLOW features at: /content/local_data/precomputed_flow_features\n",
            "--- Main CNN-LSTM Training Script (with 30-frame sequences) ---\n",
            "--- Config: Device=cuda, SeqLen=30, Batch=8, Epochs=10, LR=5e-06, WD=0.0005, Dropout=0.5\n",
            "Data Dirs (Local): Real='/content/local_data/real_frames', Fake='/content/local_data/fake_frames', Flow='/content/local_data/precomputed_flow_features'\n",
            "Freeze CNN: True\n",
            "\n",
            "--- Loading Frame Paths from local storage ---\n",
            "Frames: Real=175840, Fake=161107\n",
            "\n",
            "--- Grouping Frames and Creating Sequences ---\n",
            "Groups: Real=2931, Fake=2586\n",
            "Total sequences created: 11119 (R:5846, F:5273)\n",
            "\n",
            "--- Balancing & Splitting Data ---\n",
            "Balancing to 5273 per class.\n",
            "Balanced sequences: 10546\n",
            "Split sizes: Train=8436, Val=1055, Test=1055\n",
            "DataLoaders ready.\n",
            "\n",
            "--- Initializing CNN-LSTM Model ---\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 171MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting CNN-LSTM Model Training ---\n",
            "--- Training CNN-LSTM for 10 epochs ---\n",
            "Optimizer: AdamW, G0_LR:5.0e-07(15p), G1_LR:5.0e-06(12p), WD:0.0005, ES:3\n",
            "E1/10, TrL:0.7726, TrAcc:0.6191, VL:0.6464, VAcc:0.7005, Dur:2126.77s, LRs:['5.0e-07', '5.0e-06']\n",
            "  Best model saved: VL=0.6464, VAcc=0.7005\n",
            "E2/10, TrL:0.6784, TrAcc:0.7671, VL:0.5260, VAcc:0.8874, Dur:2139.70s, LRs:['5.0e-07', '5.0e-06']\n",
            "  Best model saved: VL=0.5260, VAcc=0.8874\n",
            "E3/10, TrL:0.5442, TrAcc:0.8294, VL:0.4078, VAcc:0.8713, Dur:2124.55s, LRs:['5.0e-07', '5.0e-06']\n",
            "  Best model saved: VL=0.4078, VAcc=0.8713\n",
            "E4/10, TrL:0.4453, TrAcc:0.8554, VL:0.2902, VAcc:0.9381, Dur:2113.42s, LRs:['5.0e-07', '5.0e-06']\n",
            "  Best model saved: VL=0.2902, VAcc=0.9381\n",
            "E5/10, TrL:0.3839, TrAcc:0.8773, VL:0.2503, VAcc:0.9257, Dur:2131.27s, LRs:['5.0e-07', '5.0e-06']\n",
            "  Best model saved: VL=0.2503, VAcc=0.9257\n",
            "E6/10, TrL:0.3267, TrAcc:0.8902, VL:0.2728, VAcc:0.8936, Dur:2087.22s, LRs:['5.0e-07', '5.0e-06']\n",
            "E7/10, TrL:0.3099, TrAcc:0.8965, VL:0.1879, VAcc:0.9629, Dur:2096.78s, LRs:['5.0e-07', '5.0e-06']\n",
            "  Best model saved: VL=0.1879, VAcc=0.9629\n",
            "E8/10, TrL:0.3153, TrAcc:0.9030, VL:0.1698, VAcc:0.9629, Dur:2092.13s, LRs:['5.0e-07', '5.0e-06']\n",
            "  Best model saved: VL=0.1698, VAcc=0.9629\n",
            "E9/10, TrL:0.2903, TrAcc:0.9082, VL:0.2233, VAcc:0.9134, Dur:2103.11s, LRs:['5.0e-07', '5.0e-06']\n",
            "E10/10, TrL:0.2854, TrAcc:0.9154, VL:0.3493, VAcc:0.8391, Dur:2105.39s, LRs:['5.0e-07', '5.0e-06']\n",
            "Training done. Total: 21134.19s (352.24m)\n",
            "Plot saved to /content/drive/MyDrive/final_balanced_data/cnn_lstm30_output/reports_and_plots/training_history_cnn_lstm.png\n",
            "\n",
            "--- Evaluation (Best CNN-LSTM Model from: /content/drive/MyDrive/final_balanced_data/cnn_lstm30_output/best_model.pth) ---\n",
            "Best model loaded for evaluation.\n",
            "\n",
            "--- Evaluation (CNN-LSTM) ---\n",
            "Accuracy: 0.9713, Weighted F1: 0.9713\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.96      0.99      0.97       422\n",
            "        Fake       0.98      0.96      0.97       380\n",
            "\n",
            "    accuracy                           0.97       802\n",
            "   macro avg       0.97      0.97      0.97       802\n",
            "weighted avg       0.97      0.97      0.97       802\n",
            "\n",
            "CM:\n",
            "[[416   6]\n",
            " [ 17 363]]\n",
            "Eval report saved to /content/drive/MyDrive/final_balanced_data/cnn_lstm30_output/reports_and_plots/eval_report_cnn_lstm.txt\n",
            "\n",
            "--- Main CNN-LSTM script execution finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overlapping On Testing"
      ],
      "metadata": {
        "id": "yC-iFOppKiky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === VIDEO-LEVEL INFERENCE ON CROSS-DATASET (REAL + FAKE) ===\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import glob\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "from collections import defaultdict\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "\n",
        "# --- Mount Drive ---\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- Config ---\n",
        "SAVED_MODEL_PATH = \"/content/drive/MyDrive/final_balanced_data/cnn_lstm30_output/best_model.pth\"\n",
        "\n",
        "# Separate zips\n",
        "FINAL_REAL_FRAMES_ZIP = \"/content/drive/MyDrive/Test_Dataset/Real/Frames_Real.zip\"\n",
        "FINAL_FAKE_FRAMES_ZIP = \"/content/drive/MyDrive/Test_Dataset/Fake/Frames_Fake.zip\"\n",
        "FINAL_REAL_FLOW_ZIP   = \"/content/drive/MyDrive/Test_Dataset/Real/Flow_Real.zip\"\n",
        "FINAL_FAKE_FLOW_ZIP   = \"/content/drive/MyDrive/Test_Dataset/Fake/Flow_Fake.zip\"\n",
        "\n",
        "# Local unzip dirs\n",
        "FRAMES_BASE_DIR = \"/content/Frames\"\n",
        "FLOW_BASE_DIR   = \"/content/Flow\"\n",
        "\n",
        "LSTM_HIDDEN_DIM = 128\n",
        "FLOW_DIM = 2\n",
        "NUM_CLASSES = 2\n",
        "DROPOUT_RATE = 0.6\n",
        "SEQ_LENGTH = 30\n",
        "INFERENCE_STEP_SIZE = SEQ_LENGTH // 2  # 50% overlap\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Helper: unzip ---\n",
        "def unzip_to_dir(zip_path, extract_dir):\n",
        "    os.makedirs(extract_dir, exist_ok=True)\n",
        "    if len(os.listdir(extract_dir)) == 0:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "            zf.extractall(extract_dir)\n",
        "        print(f\"✅ Extracted {zip_path} -> {extract_dir}\")\n",
        "    else:\n",
        "        print(f\"ℹ️ Using existing extracted folder: {extract_dir}\")\n",
        "\n",
        "# Unzip frames and flow\n",
        "unzip_to_dir(FINAL_REAL_FRAMES_ZIP, os.path.join(FRAMES_BASE_DIR, \"Real\"))\n",
        "unzip_to_dir(FINAL_FAKE_FRAMES_ZIP, os.path.join(FRAMES_BASE_DIR, \"Fake\"))\n",
        "unzip_to_dir(FINAL_REAL_FLOW_ZIP,   os.path.join(FLOW_BASE_DIR, \"Real\"))\n",
        "unzip_to_dir(FINAL_FAKE_FLOW_ZIP,   os.path.join(FLOW_BASE_DIR, \"Fake\"))\n",
        "\n",
        "# --- Image Transform ---\n",
        "IMG_TRANSFORM = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# --- Model Definition ---\n",
        "class DeepfakeDetectionModel(nn.Module):\n",
        "    def __init__(self, cnn_output_dim=512, lstm_hidden_dim=LSTM_HIDDEN_DIM, lstm_layers=1,\n",
        "                 flow_dim=FLOW_DIM, num_classes=NUM_CLASSES, dropout_rate=DROPOUT_RATE, freeze_cnn_layers=False):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        self.cnn_backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        if freeze_cnn_layers:\n",
        "            layers_to_freeze_idx = 7\n",
        "            for i, child in enumerate(self.cnn_backbone.children()):\n",
        "                if i < layers_to_freeze_idx:\n",
        "                    for param in child.parameters():\n",
        "                        param.requires_grad = False\n",
        "\n",
        "        self.cnn_output_dim = resnet.fc.in_features\n",
        "        self.lstm = nn.LSTM(input_size=self.cnn_output_dim, hidden_size=lstm_hidden_dim,\n",
        "                            num_layers=lstm_layers, batch_first=True,\n",
        "                            dropout=dropout_rate if lstm_layers > 1 else 0)\n",
        "        pooled_dim = 2 * lstm_hidden_dim\n",
        "        self.classifier_with_flow = nn.Sequential(\n",
        "            nn.Linear(pooled_dim + flow_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "        self.classifier_no_flow = nn.Sequential(\n",
        "            nn.Linear(pooled_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, frames, flow_features_sequence):\n",
        "        batch_size, seq_len, _, _, _ = frames.size()\n",
        "        cnn_features_list = []\n",
        "        for t in range(seq_len):\n",
        "            frame_input = frames[:, t, :, :, :]\n",
        "            feat = self.cnn_backbone(frame_input)\n",
        "            feat = feat.view(batch_size, -1)\n",
        "            cnn_features_list.append(feat)\n",
        "        cnn_sequence_features = torch.stack(cnn_features_list, dim=1)\n",
        "\n",
        "        lstm_out, _ = self.lstm(cnn_sequence_features)\n",
        "        lstm_mean = torch.mean(lstm_out, dim=1)\n",
        "        lstm_max = torch.max(lstm_out, dim=1).values\n",
        "        lstm_features = torch.cat((lstm_mean, lstm_max), dim=1)\n",
        "\n",
        "        if flow_features_sequence is not None and \\\n",
        "           flow_features_sequence.ndim == 3 and \\\n",
        "           flow_features_sequence.shape[1] == seq_len:\n",
        "            aggregated_flow_features = torch.mean(flow_features_sequence, dim=1)\n",
        "            combined_features = torch.cat((lstm_features, aggregated_flow_features), dim=1)\n",
        "            return self.classifier_with_flow(combined_features)\n",
        "        else:\n",
        "            return self.classifier_no_flow(lstm_features)\n",
        "\n",
        "# --- Load Model ---\n",
        "model = DeepfakeDetectionModel().to(DEVICE)\n",
        "model.load_state_dict(torch.load(SAVED_MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "print(\"✅ Model loaded successfully.\")\n",
        "\n",
        "# --- Helper: Load sequence of frames ---\n",
        "def load_frame_sequence(frame_paths):\n",
        "    imgs = [IMG_TRANSFORM(Image.open(p).convert(\"RGB\")) for p in frame_paths]\n",
        "    return torch.stack(imgs)\n",
        "\n",
        "# --- Collect sequences with overlapping ---\n",
        "def collect_sequences(label):\n",
        "    frame_dirs = sorted(glob.glob(os.path.join(FRAMES_BASE_DIR, label, \"*\")))\n",
        "    sequences = defaultdict(list)\n",
        "    for vid_dir in frame_dirs:\n",
        "        vid_name = Path(vid_dir).stem\n",
        "        frame_paths = sorted(glob.glob(os.path.join(vid_dir, \"*.jpg\")))\n",
        "        for seq_idx in range(0, len(frame_paths) - SEQ_LENGTH + 1, INFERENCE_STEP_SIZE):\n",
        "            seq_frames = frame_paths[seq_idx: seq_idx + SEQ_LENGTH]\n",
        "            if len(seq_frames) == SEQ_LENGTH:\n",
        "                flow_name = f\"{label.lower()}_{vid_name}_seq{seq_idx//INFERENCE_STEP_SIZE}.pt\"\n",
        "                flow_path = os.path.join(FLOW_BASE_DIR, label, flow_name)\n",
        "                if os.path.exists(flow_path):\n",
        "                    sequences[vid_name].append((seq_frames, flow_path, 0 if label==\"Real\" else 1))\n",
        "    return sequences\n",
        "\n",
        "print(\"📂 Collecting sequences...\")\n",
        "real_sequences = collect_sequences(\"Real\")\n",
        "fake_sequences = collect_sequences(\"Fake\")\n",
        "\n",
        "# --- Hybrid decision rule ---\n",
        "def hybrid_video_decision(seq_outputs, avg_threshold=0.45, seq_threshold=0.55):\n",
        "    \"\"\"\n",
        "    seq_outputs: list of raw logits from model for each sequence\n",
        "    avg_threshold: threshold on average fake probability\n",
        "    seq_threshold: threshold for any single sequence\n",
        "    \"\"\"\n",
        "    probs_list = [torch.softmax(out, dim=1).squeeze().cpu().numpy() for out in seq_outputs]\n",
        "    fake_probs = [p[1] for p in probs_list]\n",
        "\n",
        "    avg_fake = np.mean(fake_probs)\n",
        "    any_fake = any(p >= seq_threshold for p in fake_probs)\n",
        "\n",
        "    if avg_fake >= avg_threshold or any_fake:\n",
        "        return 1  # Fake\n",
        "    else:\n",
        "        return 0  # Real\n",
        "\n",
        "# --- Run inference ---\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for vid_dict, label_name in [(real_sequences, \"Real\"), (fake_sequences, \"Fake\")]:\n",
        "        for vid_name, seq_list in vid_dict.items():\n",
        "            true_label = 0 if label_name == \"Real\" else 1\n",
        "            seq_outputs = []\n",
        "            for frame_paths, flow_path, _ in seq_list:\n",
        "                frames_tensor = load_frame_sequence(frame_paths).unsqueeze(0).to(DEVICE)\n",
        "                flow_tensor = torch.load(flow_path).unsqueeze(0).to(DEVICE)\n",
        "                outputs = model(frames_tensor, flow_tensor)\n",
        "                seq_outputs.append(outputs)\n",
        "            if seq_outputs:\n",
        "                final_pred = hybrid_video_decision(seq_outputs, avg_threshold=0.4, seq_threshold=0.6)\n",
        "                y_true.append(true_label)\n",
        "                y_pred.append(final_pred)\n",
        "\n",
        "# --- Metrics ---\n",
        "print(\"\\n--- FINAL EVALUATION RESULTS ---\")\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "print(f\"Overall Accuracy: {acc:.4f} ({acc:.2%})\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Real\", \"Fake\"]))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Real\",\"Fake\"], yticklabels=[\"Real\",\"Fake\"])\n",
        "plt.title(\"Confusion Matrix - Video-Level Cross Dataset Inference\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wjASlb_iA9p",
        "outputId": "d942c6f5-6d16-4feb-a898-ca34e0b0a1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "ℹ️ Using existing extracted folder: /content/Frames/Real\n",
            "ℹ️ Using existing extracted folder: /content/Frames/Fake\n",
            "ℹ️ Using existing extracted folder: /content/Flow/Real\n",
            "ℹ️ Using existing extracted folder: /content/Flow/Fake\n",
            "✅ Model loaded successfully.\n",
            "📂 Collecting sequences...\n",
            "\n",
            "--- FINAL EVALUATION RESULTS ---\n",
            "Overall Accuracy: 0.8400 (84.00%)\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.78      0.96      0.86       250\n",
            "        Fake       0.94      0.72      0.82       250\n",
            "\n",
            "    accuracy                           0.84       500\n",
            "   macro avg       0.86      0.84      0.84       500\n",
            "weighted avg       0.86      0.84      0.84       500\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing on Unknown Video"
      ],
      "metadata": {
        "id": "saBlHOUQAXLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === SINGLE VIDEO UPLOAD + ON-THE-FLY INFERENCE (ALL FRAMES, SEQ=30) ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab import drive, files\n",
        "\n",
        "# --- Config ---\n",
        "SAVED_MODEL_PATH = \"/content/drive/MyDrive/final_balanced_data/cnn_lstm30_output/best_model.pth\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEQ_LENGTH = 30\n",
        "LSTM_HIDDEN_DIM = 128\n",
        "FLOW_DIM = 2\n",
        "NUM_CLASSES = 2\n",
        "DROPOUT_RATE = 0.6\n",
        "OVERLAP = 15  # frames overlap between sequences\n",
        "IMG_TRANSFORM = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
        "                         std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# --- Model definition ---\n",
        "class DeepfakeDetectionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        self.cnn_backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        self.cnn_output_dim = resnet.fc.in_features\n",
        "        self.lstm = nn.LSTM(input_size=self.cnn_output_dim, hidden_size=LSTM_HIDDEN_DIM,\n",
        "                            batch_first=True)\n",
        "        pooled_dim = 2 * LSTM_HIDDEN_DIM\n",
        "        self.classifier_with_flow = nn.Sequential(\n",
        "            nn.Linear(pooled_dim + FLOW_DIM, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROPOUT_RATE),\n",
        "            nn.Linear(128, NUM_CLASSES)\n",
        "        )\n",
        "        self.classifier_no_flow = nn.Sequential(\n",
        "            nn.Linear(pooled_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(DROPOUT_RATE),\n",
        "            nn.Linear(128, NUM_CLASSES)\n",
        "        )\n",
        "\n",
        "    def forward(self, frames, flow_features_sequence=None):\n",
        "        batch_size, seq_len, _, _, _ = frames.size()\n",
        "        cnn_feats = []\n",
        "        for t in range(seq_len):\n",
        "            x = frames[:, t]\n",
        "            feat = self.cnn_backbone(x)\n",
        "            feat = feat.view(batch_size, -1)\n",
        "            cnn_feats.append(feat)\n",
        "        cnn_sequence = torch.stack(cnn_feats, dim=1)\n",
        "        lstm_out, _ = self.lstm(cnn_sequence)\n",
        "        lstm_mean = torch.mean(lstm_out, dim=1)\n",
        "        lstm_max = torch.max(lstm_out, dim=1).values\n",
        "        lstm_feat = torch.cat([lstm_mean, lstm_max], dim=1)\n",
        "        if flow_features_sequence is not None:\n",
        "            aggregated_flow = torch.mean(flow_features_sequence, dim=1)\n",
        "            combined = torch.cat([lstm_feat, aggregated_flow], dim=1)\n",
        "            return self.classifier_with_flow(combined)\n",
        "        else:\n",
        "            return self.classifier_no_flow(lstm_feat)\n",
        "\n",
        "# --- Load model ---\n",
        "model = DeepfakeDetectionModel().to(DEVICE)\n",
        "model.load_state_dict(torch.load(SAVED_MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "print(\"✅ Model loaded successfully.\")\n",
        "\n",
        "# --- Optical flow ---\n",
        "def compute_optical_flow(frames):\n",
        "    flows = [np.array([0.0, 0.0], dtype=np.float32)]\n",
        "    for i in range(len(frames)-1):\n",
        "        prev_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
        "        next_gray = cv2.cvtColor(frames[i+1], cv2.COLOR_BGR2GRAY)\n",
        "        flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None,\n",
        "                                            0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "        mag = np.sqrt(flow[...,0]**2 + flow[...,1]**2)\n",
        "        flows.append(np.array([np.mean(mag), np.var(mag)], dtype=np.float32))\n",
        "    return torch.tensor(np.array(flows), dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "# --- Frames -> Tensor ---\n",
        "def frames_to_tensor(frames_seq):\n",
        "    imgs = [IMG_TRANSFORM(Image.fromarray(cv2.cvtColor(f, cv2.COLOR_BGR2RGB))) for f in frames_seq]\n",
        "    return torch.stack(imgs).unsqueeze(0)\n",
        "\n",
        "# --- Video inference ---\n",
        "def predict_video(video_path, seq_length=SEQ_LENGTH, overlap=OVERLAP):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = [f for ret, f in iter(lambda: cap.read(), (False, None))]\n",
        "    cap.release()\n",
        "    if len(frames) < seq_length:\n",
        "        frames += [frames[-1]] * (seq_length - len(frames))\n",
        "\n",
        "    # Create overlapping sequences\n",
        "    sequences = []\n",
        "    step = seq_length - overlap\n",
        "    for start in range(0, len(frames) - seq_length + 1, step):\n",
        "        sequences.append(frames[start:start+seq_length])\n",
        "\n",
        "    seq_preds = []\n",
        "    with torch.no_grad():\n",
        "        for seq in sequences:\n",
        "            frames_tensor = frames_to_tensor(seq).to(DEVICE)\n",
        "            flow_tensor = compute_optical_flow(seq).to(DEVICE)\n",
        "            out = model(frames_tensor, flow_tensor)\n",
        "            probs = torch.softmax(out, dim=1).squeeze().cpu().numpy()\n",
        "            seq_preds.append(int(np.argmax(probs)))\n",
        "\n",
        "    # Video-level prediction: majority vote\n",
        "    final_pred = max(set(seq_preds), key=seq_preds.count)\n",
        "    return final_pred, seq_preds\n",
        "\n",
        "# --- Mount Drive & Upload ---\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "uploaded = files.upload()\n",
        "video_file = list(uploaded.keys())[0]\n",
        "\n",
        "# --- Predict ---\n",
        "label, seqs = predict_video(video_file)\n",
        "print(f\"Predicted class: {label} (0=Real, 1=Fake)\")\n",
        "print(f\"Sequence-level predictions: {seqs}\")\n"
      ],
      "metadata": {
        "id": "_vyN5DFM8Vk-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "618ff6af-1511-48d6-cfa5-102dfd7abe75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully.\n",
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0bf4dfd5-3b5f-4799-8d05-f5b02670a521\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0bf4dfd5-3b5f-4799-8d05-f5b02670a521\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1 (12).mp4 to 1 (12).mp4\n",
            "Predicted class: 0 (0=Real, 1=Fake)\n",
            "Sequence-level predictions: [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    }
  ]
}