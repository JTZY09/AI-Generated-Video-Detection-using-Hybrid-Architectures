{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Ensure the mount point is empty before mounting\n",
        "mountpoint = '/content/drive'\n",
        "\n",
        "# Check if the mountpoint exists and is not empty\n",
        "if os.path.exists(mountpoint) and os.path.isdir(mountpoint) and os.listdir(mountpoint):\n",
        "    print(f\"Mountpoint '{mountpoint}' is not empty. Attempting to clear...\")\n",
        "    try:\n",
        "        # Use shell command for recursive removal, typically faster and more robust\n",
        "        !rm -rf {mountpoint}/*\n",
        "        print(f\"Cleared contents of '{mountpoint}'.\")\n",
        "        # Recreate the directory if it was removed by rm -rf *\n",
        "        os.makedirs(mountpoint, exist_ok=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error clearing mountpoint '{mountpoint}': {e}\")\n",
        "        # If clearing fails, you might need manual intervention or skip mounting\n",
        "        # For this case, we'll proceed with mounting, but be aware it might still fail\n",
        "        pass # Or handle the error appropriately\n",
        "\n",
        "# Now mount Google Drive\n",
        "drive.mount(mountpoint , force_remount=True)"
      ],
      "metadata": {
        "id": "0aA8ZbhLtAMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === The New, Fast, and Space-Efficient Data Setup Script ===\n",
        "# This version copies zips to a temp area and deletes each zip\n",
        "# immediately after unzipping to manage disk space.\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Configuration ---\n",
        "DRIVE_ZIPPED_DATA_DIR = \"/content/drive/MyDrive/zipped\"\n",
        "LOCAL_DATASET_DIR = \"/content/local_data\"\n",
        "# A dedicated temporary directory for the zip files\n",
        "LOCAL_TEMP_ZIP_DIR = \"/content/temp_zips\"\n",
        "# --- End of Configuration ---\n",
        "\n",
        "\n",
        "# --- Main Logic ---\n",
        "print(\"--- Starting Fast & Space-Efficient Data Setup ---\")\n",
        "start_time_total = time.time()\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Define the final local paths for each data type\n",
        "local_real_dir = os.path.join(LOCAL_DATASET_DIR, \"real_frames\")\n",
        "local_fake_dir = os.path.join(LOCAL_DATASET_DIR, \"fake_frames\")\n",
        "local_flow_dir = os.path.join(LOCAL_DATASET_DIR, \"precomputed_flow_features\")\n",
        "\n",
        "# Create the main local data directory and subdirectories\n",
        "os.makedirs(local_real_dir, exist_ok=True)\n",
        "os.makedirs(local_fake_dir, exist_ok=True)\n",
        "os.makedirs(local_flow_dir, exist_ok=True)\n",
        "# NEW: Create the dedicated temporary directory for zip files\n",
        "os.makedirs(LOCAL_TEMP_ZIP_DIR, exist_ok=True)\n",
        "print(f\"Local data directories created at '{LOCAL_DATASET_DIR}'\")\n",
        "\n",
        "\n",
        "# --- Step 1: Copy all zipped chunks into the temporary local directory ---\n",
        "print(f\"\\n--- Step 1: Copying all zip chunks from Drive to '{LOCAL_TEMP_ZIP_DIR}'... ---\")\n",
        "start_time_copy = time.time()\n",
        "\n",
        "source_zip_pattern = os.path.join(DRIVE_ZIPPED_DATA_DIR, \"*.zip\")\n",
        "zip_files_to_copy = glob.glob(source_zip_pattern)\n",
        "\n",
        "if not zip_files_to_copy:\n",
        "    print(f\"❌ ERROR: No .zip files found using glob at '{source_zip_pattern}'. Please re-check the path.\")\n",
        "else:\n",
        "    print(f\"Found {len(zip_files_to_copy)} zip files to copy.\")\n",
        "    for source_path in zip_files_to_copy:\n",
        "        # The destination is now the dedicated temp directory\n",
        "        shutil.copy(source_path, LOCAL_TEMP_ZIP_DIR)\n",
        "\n",
        "    end_time_copy = time.time()\n",
        "    print(f\"-> Copying all zips finished in {end_time_copy - start_time_copy:.2f} seconds.\")\n",
        "\n",
        "\n",
        "# --- Step 2: Unzip, Combine, and Incrementally Delete the Chunks ---\n",
        "print(\"\\n--- Step 2: Unzipping chunks and deleting zips to save space... ---\")\n",
        "\n",
        "# A) Process 'real_frames' chunks\n",
        "# The glob pattern now looks inside the temp directory\n",
        "real_zips = sorted(glob.glob(os.path.join(LOCAL_TEMP_ZIP_DIR, \"real_frames_chunk_*.zip\")))\n",
        "if real_zips:\n",
        "    print(f\"  Unzipping {len(real_zips)} 'real_frames' chunks into '{local_real_dir}'...\")\n",
        "    for zip_file in real_zips:\n",
        "        # Unzip the file\n",
        "        !unzip -q -o \"{zip_file}\" -d \"{local_real_dir}\"\n",
        "        # Immediately delete the zip file to free up space\n",
        "        os.remove(zip_file)\n",
        "        print(f\"    - Unzipped and deleted {os.path.basename(zip_file)}\")\n",
        "else:\n",
        "    print(\"  Warning: No 'real_frames' zip chunks found in temp directory.\")\n",
        "\n",
        "# B) Process 'fake_frames' chunks\n",
        "fake_zips = sorted(glob.glob(os.path.join(LOCAL_TEMP_ZIP_DIR, \"fake_frames_chunk_*.zip\")))\n",
        "if fake_zips:\n",
        "    print(f\"  Unzipping {len(fake_zips)} 'fake_frames' chunks into '{local_fake_dir}'...\")\n",
        "    for zip_file in fake_zips:\n",
        "        !unzip -q -o \"{zip_file}\" -d \"{local_fake_dir}\"\n",
        "        os.remove(zip_file)\n",
        "        print(f\"    - Unzipped and deleted {os.path.basename(zip_file)}\")\n",
        "else:\n",
        "    print(\"  Warning: No 'fake_frames' zip chunks found in temp directory.\")\n",
        "\n",
        "# C) Process 'precomputed_flow' chunk\n",
        "flow_zips = sorted(glob.glob(os.path.join(LOCAL_TEMP_ZIP_DIR, \"Combined_Precomputed_Flow*.zip\")))\n",
        "if flow_zips:\n",
        "    print(f\"  Unzipping {len(flow_zips)} 'precomputed_flow' chunk(s) into '{local_flow_dir}'...\")\n",
        "    for zip_file in flow_zips:\n",
        "        !unzip -q -o \"{zip_file}\" -d \"{local_flow_dir}\"\n",
        "        os.remove(zip_file)\n",
        "        print(f\"    - Unzipped and deleted {os.path.basename(zip_file)}\")\n",
        "else:\n",
        "    print(\"  Warning: No 'precomputed_flow' zips found in temp directory.\")\n",
        "\n",
        "\n",
        "# --- Step 3: Final Verification and Cleanup ---\n",
        "print(\"\\n--- Step 3: Verifying and cleaning up... ---\")\n",
        "try:\n",
        "    num_real = len(os.listdir(local_real_dir))\n",
        "    num_fake = len(os.listdir(local_fake_dir))\n",
        "    num_flow = len(os.listdir(local_flow_dir))\n",
        "\n",
        "    print(f\"  Verification successful:\")\n",
        "    print(f\"  - Real frames folder: {num_real} files\")\n",
        "    print(f\"  - Fake frames folder: {num_fake} files\")\n",
        "    print(f\"  - Flow features folder: {num_flow} files\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"  Verification failed. A directory was not created correctly: {e}\")\n",
        "\n",
        "# Clean up the (now empty) temporary directory for the zips\n",
        "print(f\"  Cleaning up temporary directory: '{LOCAL_TEMP_ZIP_DIR}'...\")\n",
        "shutil.rmtree(LOCAL_TEMP_ZIP_DIR)\n",
        "\n",
        "end_time_total = time.time()\n",
        "print(f\"\\n✅ SUCCESS! Your dataset is ready for training in {end_time_total - start_time_total:.2f} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9vyFW11tAXJ",
        "outputId": "e30cd9cb-97b7-474d-b9f9-06473f3dee8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Fast & Space-Efficient Data Setup ---\n",
            "Mounted at /content/drive\n",
            "Local data directories created at '/content/local_data'\n",
            "\n",
            "--- Step 1: Copying all zip chunks from Drive to '/content/temp_zips'... ---\n",
            "Found 39 zip files to copy.\n",
            "-> Copying all zips finished in 631.93 seconds.\n",
            "\n",
            "--- Step 2: Unzipping chunks and deleting zips to save space... ---\n",
            "  Unzipping 19 'real_frames' chunks into '/content/local_data/real_frames'...\n",
            "    - Unzipped and deleted real_frames_chunk_01_000000-009999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_02_010000-019999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_03_020000-029999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_04_030000-039999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_05_040000-049999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_06_050000-059999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_07_060000-069999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_08_070000-079999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_09_080000-089999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_10_090000-099999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_11_100000-109999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_12_110000-119999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_13_120000-129999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_14_130000-139999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_15_140000-149999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_16_150000-159999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_17_160000-169999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_18_170000-179999.zip\n",
            "    - Unzipped and deleted real_frames_chunk_19_180000-189999.zip\n",
            "  Unzipping 19 'fake_frames' chunks into '/content/local_data/fake_frames'...\n",
            "    - Unzipped and deleted fake_frames_chunk_01_000000-009999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_02_010000-019999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_03_020000-029999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_04_030000-039999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_05_040000-049999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_06_050000-059999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_07_060000-069999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_08_070000-079999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_09_080000-089999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_10_090000-099999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_11_100000-109999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_12_110000-119999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_13_120000-129999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_14_130000-133999.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_15_134000-139899.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_16_139900-149899.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_17_149900-159899.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_18_159900-169899.zip\n",
            "    - Unzipped and deleted fake_frames_chunk_19_169900-170199.zip\n",
            "  Unzipping 1 'precomputed_flow' chunk(s) into '/content/local_data/precomputed_flow_features'...\n",
            "    - Unzipped and deleted Combined_Precomputed_Flow.zip\n",
            "\n",
            "--- Step 3: Verifying and cleaning up... ---\n",
            "  Verification successful:\n",
            "  - Real frames folder: 175840 files\n",
            "  - Fake frames folder: 161107 files\n",
            "  - Flow features folder: 9272 files\n",
            "  Cleaning up temporary directory: '/content/temp_zips'...\n",
            "\n",
            "✅ SUCCESS! Your dataset is ready for training in 1063.43 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import glob\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import gc\n",
        "import re\n",
        "from PIL import Image\n",
        "import time\n",
        "import math\n",
        "\n",
        "# --- Debug Flag ---\n",
        "DEBUG_VERBOSE = False\n",
        "\n",
        "# --- Global Configurations ---\n",
        "SEQUENCE_LENGTH = 30\n",
        "# CNN-ViT Specific Hyperparameters\n",
        "VIT_EMBED_DIM = 768\n",
        "VIT_NUM_HEADS = 12\n",
        "VIT_NUM_LAYERS = 6 # Start with fewer layers for stability/speed\n",
        "VIT_MLP_DIM = 3072 # Typically 4 * VIT_EMBED_DIM\n",
        "FLOW_DIM = 2\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "# --- PATHS ---\n",
        "# Define local Colab destination paths (where the script will read from after copying)\n",
        "# --- PATHS (ASSUMING DATA IS ALREADY COPIED LOCALLY) ---\n",
        "LOCAL_REAL_FRAMES_DIR = '/content/local_data/real_frames'\n",
        "LOCAL_FAKE_FRAMES_DIR = '/content/local_data/fake_frames'\n",
        "PRECOMPUTED_FLOW_DIR_FOR_DATASET = '/content/local_data/precomputed_flow_features'\n",
        "\n",
        "\n",
        "# Output Paths (on Google Drive)\n",
        "DRIVE_OUTPUT_BASE = f'/content/drive/MyDrive/final_balanced_data/cnn__vit_SL{SEQUENCE_LENGTH}_output' # Added v2 for new run\n",
        "LOG_FILE_PATH = os.path.join(DRIVE_OUTPUT_BASE, 'training_log_cnn_vit.txt')\n",
        "BEST_MODEL_SAVE_PATH = os.path.join(DRIVE_OUTPUT_BASE, f'best_model_cnn_vit_L{VIT_NUM_LAYERS}.pth')\n",
        "OUTPUT_DIR_FOR_PLOTS_AND_REPORTS = os.path.join(DRIVE_OUTPUT_BASE, 'reports_and_plots_cnn_vit')\n",
        "\n",
        "# Fallback local paths for outputs if Drive fails\n",
        "LOCAL_OUTPUT_BASE_FALLBACK = f'/content/outputs_local_cnn_vit_SL{SEQUENCE_LENGTH}_v2'\n",
        "_LOG_FILE_PATH_TEMP = os.path.join(LOCAL_OUTPUT_BASE_FALLBACK, 'training_log_cnn_vit.txt')\n",
        "_BEST_MODEL_SAVE_PATH_TEMP = os.path.join(LOCAL_OUTPUT_BASE_FALLBACK, f'best_model_cnn_vit_L{VIT_NUM_LAYERS}.pth')\n",
        "_OUTPUT_DIR_FOR_PLOTS_AND_REPORTS_TEMP = os.path.join(LOCAL_OUTPUT_BASE_FALLBACK, 'reports_and_plots_cnn_vit')\n",
        "\n",
        "# --- Mount Drive and Setup Output Dirs ---\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Google Drive mounted.\")\n",
        "    # Use the actual DRIVE_OUTPUT_BASE for creating directories\n",
        "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(BEST_MODEL_SAVE_PATH), exist_ok=True)\n",
        "    os.makedirs(OUTPUT_DIR_FOR_PLOTS_AND_REPORTS, exist_ok=True)\n",
        "    print(f\"Output directories on Drive ensured/created at {DRIVE_OUTPUT_BASE}\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Drive mount/setup error: {e}. Using local fallback for outputs.\")\n",
        "    # Switch global output paths to local fallback versions\n",
        "    DRIVE_OUTPUT_BASE = LOCAL_OUTPUT_BASE_FALLBACK # This redefines the base for subsequent path joinings\n",
        "    LOG_FILE_PATH = _LOG_FILE_PATH_TEMP\n",
        "    BEST_MODEL_SAVE_PATH = _BEST_MODEL_SAVE_PATH_TEMP\n",
        "    OUTPUT_DIR_FOR_PLOTS_AND_REPORTS = _OUTPUT_DIR_FOR_PLOTS_AND_REPORTS_TEMP\n",
        "    os.makedirs(os.path.dirname(LOG_FILE_PATH), exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(BEST_MODEL_SAVE_PATH), exist_ok=True)\n",
        "    os.makedirs(OUTPUT_DIR_FOR_PLOTS_AND_REPORTS, exist_ok=True)\n",
        "    print(f\"Using LOCAL fallback paths for outputs: {DRIVE_OUTPUT_BASE}\")\n",
        "\n",
        "\n",
        "# --- PositionalEncoding Class ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=50): # max_len should be >= sequence_length + 1 (for CLS)\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len, d_model]\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# --- DeepfakeDetectionModel_CNNViT Class ---\n",
        "class DeepfakeDetectionModel_CNNViT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 cnn_output_dim_from_resnet=512,\n",
        "                 vit_embed_dim=VIT_EMBED_DIM, # Use global\n",
        "                 vit_num_heads=VIT_NUM_HEADS, # Use global\n",
        "                 vit_num_layers=VIT_NUM_LAYERS, # Use global\n",
        "                 vit_mlp_dim=VIT_MLP_DIM, # Use global\n",
        "                 sequence_length=SEQUENCE_LENGTH, # Use global\n",
        "                 flow_dim=FLOW_DIM, # Use global\n",
        "                 num_classes=NUM_CLASSES, # Use global\n",
        "                 dropout_rate=0.1, # Default dropout for ViT parts\n",
        "                 freeze_cnn_layers=False):\n",
        "        super(DeepfakeDetectionModel_CNNViT, self).__init__()\n",
        "\n",
        "        _resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        print(\"INFO: Initializing ResNet18 CNN with ImageNet pretrained weights for CNN-ViT.\")\n",
        "        # Create cnn_backbone by removing the final FC layer\n",
        "        self.cnn_backbone = nn.Sequential(*list(_resnet.children())[:-1]) # Output is 512-dim after avgpool\n",
        "\n",
        "        if freeze_cnn_layers:\n",
        "            # Children of self.cnn_backbone:\n",
        "            # 0:conv1, 1:bn1, 2:relu, 3:maxpool, 4:layer1, 5:layer2, 6:layer3, 7:layer4, 8:avgpool\n",
        "            layers_to_freeze_until_idx = 6 # Freeze conv1 through layer2 (i.e., children 0 to 5)\n",
        "            for i, child in enumerate(self.cnn_backbone.children()):\n",
        "                if i < layers_to_freeze_until_idx:\n",
        "                    for param in child.parameters():\n",
        "                        param.requires_grad = False\n",
        "            print(f\"INFO: Frozen ResNet18 layers in cnn_backbone up to child index {layers_to_freeze_until_idx-1}.\")\n",
        "\n",
        "        self.cnn_feature_proj = nn.Linear(cnn_output_dim_from_resnet, vit_embed_dim) \\\n",
        "            if cnn_output_dim_from_resnet != vit_embed_dim else nn.Identity()\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, vit_embed_dim))\n",
        "        self.pos_encoder = PositionalEncoding(vit_embed_dim, max_len=sequence_length + 1) # +1 for CLS\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=vit_embed_dim, nhead=vit_num_heads, dim_feedforward=vit_mlp_dim,\n",
        "            dropout=dropout_rate, batch_first=True, activation='gelu'\n",
        "        )\n",
        "        self.vit_encoder = nn.TransformerEncoder(encoder_layer, num_layers=vit_num_layers)\n",
        "\n",
        "        self.flow_dim = flow_dim\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(vit_embed_dim + self.flow_dim),\n",
        "            nn.Linear(vit_embed_dim + self.flow_dim, vit_embed_dim // 2), nn.GELU(),\n",
        "            nn.Dropout(dropout_rate), # Use the same dropout rate here\n",
        "            nn.Linear(vit_embed_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, frames, flow_features_sequence):\n",
        "        batch_size, seq_len, C, H, W = frames.size()\n",
        "        cnn_features_list = []\n",
        "        for t in range(seq_len):\n",
        "            frame_input = frames[:, t, :, :, :]\n",
        "            feat = self.cnn_backbone(frame_input)\n",
        "            feat = feat.view(batch_size, -1)\n",
        "            cnn_features_list.append(feat)\n",
        "        cnn_sequence_features = torch.stack(cnn_features_list, dim=1)\n",
        "\n",
        "        vit_input_features = self.cnn_feature_proj(cnn_sequence_features)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat((cls_tokens, vit_input_features), dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "        vit_encoded_output = self.vit_encoder(x)\n",
        "        vit_sequence_output = vit_encoded_output[:, 0]\n",
        "\n",
        "        if flow_features_sequence.ndim == 3 and flow_features_sequence.shape[1] == seq_len:\n",
        "            aggregated_flow_features = torch.mean(flow_features_sequence, dim=1)\n",
        "        elif flow_features_sequence.ndim == 2 and flow_features_sequence.shape[0] == batch_size:\n",
        "            aggregated_flow_features = flow_features_sequence\n",
        "        else:\n",
        "            aggregated_flow_features = torch.zeros(batch_size, self.flow_dim, device=frames.device)\n",
        "\n",
        "        combined_features = torch.cat((vit_sequence_output, aggregated_flow_features), dim=1)\n",
        "        output = self.classifier(combined_features)\n",
        "        return output\n",
        "\n",
        "# --- DeepfakeFrameDataset Class (Loads flow for REAL and FAKE) ---\n",
        "class DeepfakeFrameDataset_ViT(Dataset):\n",
        "    def __init__(self, sequences_info_list, sequence_length=SEQUENCE_LENGTH, transform=None):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.transform = transform\n",
        "        # Filter valid samples\n",
        "        self.sequences_info = [info for info in sequences_info_list if info is not None and info[0] is not None]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self.sequences_info):\n",
        "            return None, None, None\n",
        "\n",
        "        seq_paths, label, sequence_identifier = self.sequences_info[idx]\n",
        "        if not isinstance(seq_paths, list) or len(seq_paths) != self.sequence_length:\n",
        "            return None, None, None\n",
        "\n",
        "        # --- Frame loading ---\n",
        "        frames = []\n",
        "        for path_str in seq_paths:\n",
        "            try:\n",
        "                img = Image.open(str(path_str)).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                frames.append(img)\n",
        "            except:\n",
        "                return None, None, None\n",
        "\n",
        "        if len(frames) != self.sequence_length:\n",
        "            return None, None, None\n",
        "\n",
        "        frames_tensor = torch.stack(frames)\n",
        "\n",
        "        # --- Flow loading ---\n",
        "        flow_feature_filename = f\"{sequence_identifier}_flow.pt\"\n",
        "        flow_feature_path = os.path.join(PRECOMPUTED_FLOW_DIR_FOR_DATASET, flow_feature_filename)\n",
        "\n",
        "        flow_features_sequence = None\n",
        "        try:\n",
        "            loaded_flow = torch.load(flow_feature_path)\n",
        "            if loaded_flow.shape == (self.sequence_length, 2):\n",
        "                flow_features_sequence = loaded_flow\n",
        "            else:\n",
        "                return None, None, None\n",
        "        except:\n",
        "            return None, None, None\n",
        "\n",
        "        return frames_tensor, flow_features_sequence, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# --- Training Function (with revised parameter grouping) ---\n",
        "def train_model(model, train_loader, val_loader, device, sequence_length,\n",
        "                num_epochs=10, accum_steps=2, learning_rate=0.0005, weight_decay=1e-5):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    cnn_backbone_params_list = []\n",
        "    if hasattr(model, 'cnn_backbone'):\n",
        "        cnn_backbone_params_list = [p for p in model.cnn_backbone.parameters() if p.requires_grad]\n",
        "\n",
        "    vit_parts_params_list = []\n",
        "    if hasattr(model, 'cnn_feature_proj'):\n",
        "        vit_parts_params_list.extend([p for p in model.cnn_feature_proj.parameters() if p.requires_grad])\n",
        "    if hasattr(model, 'cls_token') and model.cls_token.requires_grad:\n",
        "        vit_parts_params_list.append(model.cls_token)\n",
        "    if hasattr(model, 'vit_encoder'):\n",
        "        vit_parts_params_list.extend([p for p in model.vit_encoder.parameters() if p.requires_grad])\n",
        "\n",
        "    classifier_params_list = []\n",
        "    if hasattr(model, 'classifier'):\n",
        "        classifier_params_list = [p for p in model.classifier.parameters() if p.requires_grad]\n",
        "\n",
        "    # Collect IDs of already assigned params to find remaining ones\n",
        "    assigned_param_ids = set()\n",
        "    for p_list in [cnn_backbone_params_list, vit_parts_params_list, classifier_params_list]:\n",
        "        for p in p_list:\n",
        "            assigned_param_ids.add(id(p))\n",
        "\n",
        "    remaining_params_list = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad and id(param) not in assigned_param_ids:\n",
        "            # print(f\"  Parameter '{name}' not in main groups, adding to 'remaining_params_list'\")\n",
        "            remaining_params_list.append(param)\n",
        "\n",
        "    optimizer_grouped_parameters = []\n",
        "    if cnn_backbone_params_list:\n",
        "        optimizer_grouped_parameters.append({'params': cnn_backbone_params_list, 'lr': learning_rate / 10})\n",
        "    if vit_parts_params_list:\n",
        "        optimizer_grouped_parameters.append({'params': vit_parts_params_list, 'lr': learning_rate})\n",
        "    if classifier_params_list:\n",
        "        optimizer_grouped_parameters.append({'params': classifier_params_list, 'lr': learning_rate * 2}) # Example: higher LR for new head\n",
        "    if remaining_params_list:\n",
        "        print(f\"Warning: Adding {len(remaining_params_list)} 'remaining' trainable parameters to optimizer with main LR ({learning_rate}).\")\n",
        "        optimizer_grouped_parameters.append({'params': remaining_params_list, 'lr': learning_rate})\n",
        "\n",
        "    optimizer_grouped_parameters = [pg for pg in optimizer_grouped_parameters if pg['params']] # Remove empty groups\n",
        "\n",
        "    if not optimizer_grouped_parameters:\n",
        "        print(\"Warning: No trainable parameters found for grouped optimizer. Using all model parameters with main LR.\")\n",
        "        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, weight_decay=weight_decay)\n",
        "    else:\n",
        "        optimizer = optim.AdamW(optimizer_grouped_parameters, weight_decay=weight_decay)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=4)\n",
        "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'epoch_time': []}\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    patience_early_stop = 4\n",
        "\n",
        "    print(f\"--- train_model (CNN-ViT): Starting training for {num_epochs} epochs ---\")\n",
        "    lrs_to_print = []\n",
        "    for pg_idx, pg in enumerate(optimizer.param_groups):\n",
        "        lrs_to_print.append(f\"Group{pg_idx}_LR:{pg['lr']:.1e}({len(pg['params'])} params)\")\n",
        "    print(f\"Optimizer: AdamW, {', '.join(lrs_to_print)}, Weight Decay: {weight_decay}, Early Stop: {patience_early_stop}\")\n",
        "\n",
        "\n",
        "    total_training_start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start_time = time.time(); model.train()\n",
        "        train_loss_sum = 0.0; train_preds_epoch, train_labels_epoch = [], []; optimizer.zero_grad()\n",
        "        processed_batches_in_epoch = 0\n",
        "        for i, batch_data in enumerate(train_loader):\n",
        "            if batch_data is None or batch_data[0] is None: continue\n",
        "            frames, flow_features_sequence, labels = batch_data\n",
        "            if frames is None or flow_features_sequence is None or labels is None: continue\n",
        "            frames, flow_features_sequence, labels = frames.to(device), flow_features_sequence.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(frames, flow_features_sequence); loss = criterion(outputs, labels)\n",
        "            loss_scaled = loss / accum_steps; loss_scaled.backward()\n",
        "            if (i + 1) % accum_steps == 0 or (i + 1) == len(train_loader):\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step(); optimizer.zero_grad()\n",
        "            train_loss_sum += loss.item(); _, preds = torch.max(outputs, 1)\n",
        "            train_preds_epoch.extend(preds.cpu().numpy()); train_labels_epoch.extend(labels.cpu().numpy())\n",
        "            processed_batches_in_epoch += 1\n",
        "            del frames, flow_features_sequence, labels, outputs, loss, loss_scaled\n",
        "            if device.type == 'cuda': torch.cuda.empty_cache()\n",
        "\n",
        "        avg_train_loss = train_loss_sum / processed_batches_in_epoch if processed_batches_in_epoch > 0 else float('inf')\n",
        "        epoch_train_acc = accuracy_score(train_labels_epoch, train_preds_epoch) if train_labels_epoch else 0.0\n",
        "        history['train_loss'].append(avg_train_loss); history['train_acc'].append(epoch_train_acc)\n",
        "\n",
        "        model.eval(); val_loss_sum = 0.0; val_preds_epoch, val_labels_epoch = [], []; processed_val_batches = 0\n",
        "        if val_loader:\n",
        "            with torch.no_grad():\n",
        "                for batch_data_val in val_loader:\n",
        "                    if batch_data_val is None or batch_data_val[0] is None: continue\n",
        "                    frames, flow_features_sequence, labels = batch_data_val\n",
        "                    if frames is None or flow_features_sequence is None or labels is None: continue\n",
        "                    frames, flow_features_sequence, labels = frames.to(device), flow_features_sequence.to(device), labels.to(device)\n",
        "                    outputs = model(frames, flow_features_sequence); loss = criterion(outputs, labels)\n",
        "                    val_loss_sum += loss.item(); _, preds = torch.max(outputs, 1)\n",
        "                    val_preds_epoch.extend(preds.cpu().numpy()); val_labels_epoch.extend(labels.cpu().numpy())\n",
        "                    processed_val_batches += 1\n",
        "                    del frames, flow_features_sequence, labels, outputs, loss\n",
        "                    if device.type == 'cuda': torch.cuda.empty_cache()\n",
        "\n",
        "        avg_val_loss = val_loss_sum / processed_val_batches if processed_val_batches > 0 else float('inf')\n",
        "        epoch_val_acc = accuracy_score(val_labels_epoch, val_preds_epoch) if val_labels_epoch else 0.0\n",
        "        history['val_loss'].append(avg_val_loss); history['val_acc'].append(epoch_val_acc)\n",
        "\n",
        "        epoch_duration = time.time() - epoch_start_time; history['epoch_time'].append(epoch_duration)\n",
        "        current_lrs_log = [f\"{g['lr']:.1e}\" for g in optimizer.param_groups]\n",
        "        log_msg = (f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}, '\n",
        "                   f'Val Loss: {avg_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}, Duration: {epoch_duration:.2f}s, LRs: {current_lrs_log}')\n",
        "        print(log_msg);\n",
        "        with open(LOG_FILE_PATH, 'a') as f: f.write(log_msg + '\\n')\n",
        "        scheduler.step(avg_val_loss)\n",
        "        if processed_val_batches > 0 and avg_val_loss < best_val_loss :\n",
        "            best_val_loss = avg_val_loss; torch.save(model.state_dict(), BEST_MODEL_SAVE_PATH)\n",
        "            print(f\"  Best model (val_loss) saved: {best_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
        "            epochs_no_improve = 0\n",
        "        elif processed_val_batches > 0: epochs_no_improve += 1\n",
        "        if epochs_no_improve >= patience_early_stop:\n",
        "            print(f\"Early stopping after {patience_early_stop} epochs with no val_loss improvement.\"); break\n",
        "        gc.collect();\n",
        "        if device.type == 'cuda': torch.cuda.empty_cache()\n",
        "    total_training_duration = time.time() - total_training_start_time\n",
        "    print(f\"Training finished. Total time: {total_training_duration:.2f}s ({total_training_duration/60:.2f} min)\")\n",
        "    with open(LOG_FILE_PATH, 'a') as f: f.write(f\"\\nTotal training time: {total_training_duration:.2f}s ({total_training_duration/3600:.2f} hr)\\n\")\n",
        "    return model, history\n",
        "\n",
        "# --- Plotting, Evaluation, Grouping, Collate Functions ---\n",
        "def plot_training_history(history, save_dir):\n",
        "    if not history or not history.get('train_loss') : print(\"Warn: History incomplete for plotting.\"); return\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
        "    color = 'tab:red'\n",
        "    ax1.set_xlabel('Epochs'); ax1.set_ylabel('Loss', color=color)\n",
        "    ax1.plot(epochs, history['train_loss'], 'bo-', label='Training Loss')\n",
        "    if history.get('val_loss') and any(v != float('inf') for v in history['val_loss']):\n",
        "        ax1.plot(epochs, history['val_loss'], 'ro-', label='Validation Loss')\n",
        "    ax1.tick_params(axis='y', labelcolor=color); ax1.legend(loc='upper left')\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:blue'\n",
        "    ax2.set_ylabel('Accuracy', color=color)\n",
        "    if history.get('train_acc'): ax2.plot(epochs, history['train_acc'], 'bs--', label='Training Acc')\n",
        "    if history.get('val_acc') and history['val_acc']: ax2.plot(epochs, history['val_acc'], 'rs--', label='Validation Acc')\n",
        "    ax2.tick_params(axis='y', labelcolor=color); ax2.legend(loc='upper right')\n",
        "    fig.tight_layout(); plt.title('CNN-ViT Training History')\n",
        "    plot_path = os.path.join(save_dir, 'training_history_cnn_vit.png'); plt.savefig(plot_path)\n",
        "    print(f\"Training history plot saved to {plot_path}\"); plt.close()\n",
        "\n",
        "# This is the function you want from cnn_lstm.py\n",
        "\n",
        "# --- Paste this complete, updated function into your cnn_vit.py script ---\n",
        "\n",
        "def evaluate_model_and_report(model, test_loader, device, output_dir): # sequence_length is not used, so we can remove it\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    print(\"\\n--- Evaluation (CNN-ViT) ---\") # <-- MODIFIED\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for frames, flows, labels in test_loader:\n",
        "            if frames is None: continue # Skip if batch is empty from collate_fn\n",
        "            frames, flows, labels = frames.to(device), flows.to(device), labels.to(device)\n",
        "            outputs = model(frames, flows)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    if not all_labels:\n",
        "        print(\"Evaluation failed: no valid samples found in the test loader.\")\n",
        "        return\n",
        "\n",
        "    # --- Metrics ---\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    report = classification_report(all_labels, all_preds, target_names=[\"Real\", \"Fake\"], zero_division=0)\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    print(f\"Accuracy: {acc:.4f}, Weighted F1: {f1:.4f}\\n{report}\\nCM:\\n{cm}\")\n",
        "\n",
        "    # --- Saving Reports and Plots ---\n",
        "    # MODIFIED filenames to be specific to CNN-ViT\n",
        "    report_path = os.path.join(output_dir, 'eval_report_cnn_vit.txt')\n",
        "    cm_path = os.path.join(output_dir, 'cm_cnn_vit.png')\n",
        "\n",
        "    with open(report_path, 'w') as f:\n",
        "        f.write(f\"Accuracy: {acc:.4f}\\n\")\n",
        "        f.write(f\"Weighted F1: {f1:.4f}\\n\\n\")\n",
        "        f.write(\"Classification Report:\\n\")\n",
        "        f.write(f\"{report}\\n\\n\")\n",
        "        f.write(\"Confusion Matrix:\\n\")\n",
        "        f.write(np.array2string(cm))\n",
        "\n",
        "    print(f\"Eval report saved to {report_path}\")\n",
        "\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=[\"Real\", \"Fake\"], yticklabels=[\"Real\", \"Fake\"])\n",
        "    plt.title('Confusion Matrix (CNN-ViT)') # <-- MODIFIED\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(cm_path)\n",
        "    plt.close()\n",
        "    print(f\"Confusion matrix plot saved to {cm_path}\")\n",
        "\n",
        "def group_frames_by_video(frame_paths, class_label_for_debug=\"\"):\n",
        "    video_groups = defaultdict(list)\n",
        "    if not frame_paths: return video_groups\n",
        "    base_real_dir_name = Path(LOCAL_REAL_FRAMES_DIR).name\n",
        "    base_fake_dir_name = Path(LOCAL_FAKE_FRAMES_DIR).name\n",
        "    for path_str in frame_paths:\n",
        "        path_obj = Path(str(path_str)); stem = path_obj.stem; video_stem = None\n",
        "        parent_name = path_obj.parent.name\n",
        "        if parent_name not in [base_real_dir_name, base_fake_dir_name] and parent_name != Path(LOCAL_REAL_FRAMES_DIR).parent.name and parent_name != Path(LOCAL_FAKE_FRAMES_DIR).parent.name :\n",
        "             video_stem = parent_name # Use subfolder name if frames are in video-specific subfolders\n",
        "        else: # Frames are likely in the base directory or subfolders that are not unique video IDs\n",
        "            if '_frame' in stem: video_stem = stem.rsplit('_frame', 1)[0]\n",
        "            elif '_f' in stem: video_stem = stem.rsplit('_f', 1)[0]\n",
        "            else:\n",
        "                match = re.match(r'^(.*?)_?(\\d{2,})$', stem)\n",
        "                video_stem = match.group(1).rstrip('_-') if match and match.group(1) else stem\n",
        "        if not video_stem or video_stem in [base_real_dir_name, base_fake_dir_name]:\n",
        "            video_stem = f\"defaultgroup_{class_label_for_debug}_{Path(path_str).name.split('.')[0]}\" # Make fallback more unique\n",
        "        video_groups[video_stem].append(str(path_str))\n",
        "    for k in list(video_groups.keys()): # Use list to avoid issues if modifying dict during iteration (though not done here)\n",
        "        try: video_groups[k] = sorted(video_groups[k], key=lambda x:int(re.search(r'(\\d+)(?!.*\\d)',Path(x).stem).group(1)))\n",
        "        except: video_groups[k] = sorted(video_groups[k]) # Fallback alphanumeric sort\n",
        "    return video_groups\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    batch = [item for item in batch if item is not None and \\\n",
        "             item[0] is not None and item[1] is not None and item[2] is not None and \\\n",
        "             item[0].shape[0] == SEQUENCE_LENGTH and item[1].shape[0] == SEQUENCE_LENGTH]\n",
        "    if not batch: return None, None, None\n",
        "    try: return torch.utils.data.dataloader.default_collate(batch)\n",
        "    except Exception as e: print(f\"Collate Error: {e}\"); return None, None, None\n",
        "\n",
        "def create_sequences_from_group(video_group, label, sequence_length=SEQUENCE_LENGTH, stride=5):\n",
        "    sequences_info = []\n",
        "    for video_id, frame_paths in video_group.items():\n",
        "        if len(frame_paths) < sequence_length:\n",
        "            continue\n",
        "        for start in range(0, len(frame_paths) - sequence_length + 1, stride):\n",
        "            seq_paths = frame_paths[start:start+sequence_length]\n",
        "            sequences_info.append((seq_paths, label, video_id))\n",
        "    return sequences_info\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    batch = [item for item in batch if item and all(t is not None for t in item) and \\\n",
        "             item[0].shape[0]==SEQUENCE_LENGTH and item[1].shape[0]==SEQUENCE_LENGTH]\n",
        "    return torch.utils.data.dataloader.default_collate(batch) if batch else (None,None,None)\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == '__main__':\n",
        "    print(\"--- Main CNN-ViT Training Script Execution Started ---\")\n",
        "    batch_size = 8\n",
        "    num_epochs = 10\n",
        "    accum_steps = 2\n",
        "    learning_rate_param = 4e-5\n",
        "    weight_decay_param = 1e-4\n",
        "    dropout_rate_param = 0.5 # For ViT parts and classifier\n",
        "    freeze_cnn_layers_initially = True # Freeze ResNet layers except last block initially\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"--- Config: Device={device}, SeqLen={SEQUENCE_LENGTH}, Batch={batch_size}, Epochs={num_epochs}, LR={learning_rate_param}, WD={weight_decay_param}, Dropout={dropout_rate_param}\")\n",
        "    print(f\"ViT Params: Embed={VIT_EMBED_DIM}, Heads={VIT_NUM_HEADS}, Layers={VIT_NUM_LAYERS}, MLP_Dim={VIT_MLP_DIM}\")\n",
        "    print(f\"Data Dirs: Real='{LOCAL_REAL_FRAMES_DIR}', Fake='{LOCAL_FAKE_FRAMES_DIR}', Flow='{PRECOMPUTED_FLOW_DIR_FOR_DATASET}'\")\n",
        "    print(f\"Freeze CNN: {freeze_cnn_layers_initially}\")\n",
        "\n",
        "        # --- Data Transforms ---\n",
        "    # Training: strong augmentation\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),   # same as LSTM\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(\n",
        "            brightness=0.1,\n",
        "            contrast=0.1,\n",
        "            saturation=0.05,\n",
        "            hue=0.02\n",
        "        ),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "        transforms.RandomErasing(\n",
        "            p=0.1,\n",
        "            scale=(0.02, 0.1),\n",
        "            ratio=(0.5, 2.0),\n",
        "            value=0\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # Validation/Test: only resize + normalize (no augmentation)\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "\n",
        "    if not (os.path.exists(LOCAL_REAL_FRAMES_DIR) and os.path.exists(LOCAL_FAKE_FRAMES_DIR)):\n",
        "        print(\"CRITICAL: Frame dirs missing. Exiting.\"); exit()\n",
        "    if not os.path.exists(PRECOMPUTED_FLOW_DIR_FOR_DATASET):\n",
        "        print(f\"CRITICAL WARNING: Flow dir missing: {PRECOMPUTED_FLOW_DIR_FOR_DATASET}. Flow features will be zeros.\")\n",
        "\n",
        "    print(f\"\\n--- Loading Frame Paths from local storage ---\")\n",
        "    img_patterns = [\"*.[jJ][pP][gG]\", \"*.[jJ][pP][eE][gG]\", \"*.[pP][nN][gG]\"]\n",
        "    all_real_frame_paths = [p for pattern in img_patterns for p in glob.glob(os.path.join(LOCAL_REAL_FRAMES_DIR, '**', pattern), recursive=True)]\n",
        "    all_fake_frame_paths = [p for pattern in img_patterns for p in glob.glob(os.path.join(LOCAL_FAKE_FRAMES_DIR, '**', pattern), recursive=True)]\n",
        "    all_real_frame_paths = sorted(list(set(all_real_frame_paths)))\n",
        "    all_fake_frame_paths = sorted(list(set(all_fake_frame_paths)))\n",
        "    print(f\"Frames: Real={len(all_real_frame_paths)}, Fake={len(all_fake_frame_paths)}\")\n",
        "    if not all_real_frame_paths and not all_fake_frame_paths: raise ValueError(\"No frames loaded.\")\n",
        "\n",
        "    print(\"\\n--- Grouping Frames by Video ---\")\n",
        "    real_video_groups = group_frames_by_video(all_real_frame_paths, \"real\")\n",
        "    fake_video_groups = group_frames_by_video(all_fake_frame_paths, \"fake\")\n",
        "    print(f\"Groups: Real={len(real_video_groups)}, Fake={len(fake_video_groups)}\")\n",
        "\n",
        "    all_sequences_info = []\n",
        "    STEP_TRAIN_SEQ = SEQUENCE_LENGTH # Non-overlapping for training sequence creation\n",
        "    for lbl_str, groups in [(\"real\", real_video_groups), (\"fake\", fake_video_groups)]:\n",
        "        lbl = 0 if lbl_str == \"real\" else 1\n",
        "        for video_stem, frames in groups.items():\n",
        "            if len(frames) < SEQUENCE_LENGTH: continue\n",
        "            for i in range(0, len(frames) - SEQUENCE_LENGTH + 1, STEP_TRAIN_SEQ):\n",
        "                seq = frames[i : i + SEQUENCE_LENGTH]\n",
        "                if len(seq) == SEQUENCE_LENGTH:\n",
        "                    safe_stem = re.sub(r'[\\\\/*?:\"<>|]',\"_\", str(video_stem))\n",
        "                    seq_id = f\"{lbl_str}_{safe_stem}_seq{i//STEP_TRAIN_SEQ}\"\n",
        "                    all_sequences_info.append((seq, lbl, seq_id))\n",
        "    num_r = sum(1 for _,l,_ in all_sequences_info if l==0); num_f = sum(1 for _,l,_ in all_sequences_info if l==1)\n",
        "    print(f\"Total sequences created: {len(all_sequences_info)} (Real: {num_r}, Fake: {num_f})\")\n",
        "    if not all_sequences_info: raise ValueError(\"No sequences created.\")\n",
        "\n",
        "    print(\"\\n--- Balancing & Splitting Data ---\")\n",
        "    if num_r > 0 and num_f > 0:\n",
        "        min_c = min(num_r, num_f)\n",
        "        real_s = random.sample([s for s in all_sequences_info if s[1]==0], min_c)\n",
        "        fake_s = random.sample([s for s in all_sequences_info if s[1]==1], min_c)\n",
        "        balanced_sequences_info = real_s + fake_s; random.shuffle(balanced_sequences_info)\n",
        "    else: balanced_sequences_info = all_sequences_info\n",
        "    print(f\"Sequences after balancing: {len(balanced_sequences_info)}\")\n",
        "    if not balanced_sequences_info: raise ValueError(\"No sequences after balancing.\")\n",
        "\n",
        "    labels_strat = [info[1] for info in balanced_sequences_info]\n",
        "    # Simplified 80/10/10 split for brevity; use your robust splitting if needed\n",
        "    train_val_info, test_dataset_info = train_test_split(balanced_sequences_info, test_size=0.1, stratify=labels_strat, random_state=42)\n",
        "    train_labels_strat = [info[1] for info in train_val_info]\n",
        "    train_dataset_info, val_dataset_info = train_test_split(train_val_info, test_size=0.111, stratify=train_labels_strat, random_state=42) # 0.111 of 0.9 is ~0.10.111 of 0.9 is ~0.1\n",
        "    print(f\"Split sizes: Train={len(train_dataset_info)}, Val={len(val_dataset_info)}, Test={len(test_dataset_info)}\")\n",
        "\n",
        "    train_dataset = DeepfakeFrameDataset_ViT(train_dataset_info, transform=train_transform)\n",
        "    val_dataset   = DeepfakeFrameDataset_ViT(val_dataset_info, transform=val_test_transform) if val_dataset_info else None\n",
        "    test_dataset  = DeepfakeFrameDataset_ViT(test_dataset_info, transform=val_test_transform) if test_dataset_info else None\n",
        "\n",
        "\n",
        "    num_workers_dl = 2 if device.type == 'cuda' else 0\n",
        "    pin_mem = True if num_workers_dl > 0 else False\n",
        "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=num_workers_dl, collate_fn=custom_collate_fn, pin_memory=pin_mem, prefetch_factor=2 if pin_mem else None)\n",
        "    val_loader = DataLoader(val_dataset, batch_size, shuffle=False, num_workers=num_workers_dl, collate_fn=custom_collate_fn, pin_memory=pin_mem, prefetch_factor=2 if pin_mem else None) if val_dataset and len(val_dataset)>0 else None\n",
        "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=num_workers_dl, collate_fn=custom_collate_fn, pin_memory=pin_mem, prefetch_factor=2 if pin_mem else None) if test_dataset and len(test_dataset)>0 else None\n",
        "    print(\"DataLoaders ready.\")\n",
        "\n",
        "    print(\"\\n--- Initializing CNN-ViT Model ---\")\n",
        "    model = DeepfakeDetectionModel_CNNViT(\n",
        "        cnn_output_dim_from_resnet=512, vit_embed_dim=VIT_EMBED_DIM, vit_num_heads=VIT_NUM_HEADS,\n",
        "        vit_num_layers=VIT_NUM_LAYERS, vit_mlp_dim=VIT_MLP_DIM, sequence_length=SEQUENCE_LENGTH,\n",
        "        flow_dim=FLOW_DIM, num_classes=NUM_CLASSES, dropout_rate=dropout_rate_param,\n",
        "        freeze_cnn_layers=freeze_cnn_layers_initially\n",
        "    ).to(device)\n",
        "\n",
        "    if train_loader and len(train_dataset) > 0:\n",
        "        print(\"\\n--- Starting CNN-ViT Model Training ---\")\n",
        "        with open(LOG_FILE_PATH, 'w') as f: f.write(f\"--- Training Log (CNN-ViT) ---\\n\")\n",
        "        model, training_history = train_model(\n",
        "            model, train_loader, val_loader, device, SEQUENCE_LENGTH, num_epochs, accum_steps,\n",
        "            learning_rate_param, weight_decay_param\n",
        "        )\n",
        "    if training_history : plot_training_history(training_history, OUTPUT_DIR_FOR_PLOTS_AND_REPORTS)\n",
        "\n",
        "    if os.path.exists(BEST_MODEL_SAVE_PATH):\n",
        "        print(f\"\\n--- Evaluation (Best CNN-ViT Model) ---\")\n",
        "        final_model = DeepfakeDetectionModel_CNNViT(\n",
        "            vit_embed_dim=VIT_EMBED_DIM, vit_num_heads=VIT_NUM_HEADS, vit_num_layers=VIT_NUM_LAYERS,\n",
        "            vit_mlp_dim=VIT_MLP_DIM, dropout_rate=dropout_rate_param).to(device) # Re-init for loading\n",
        "        try:\n",
        "            final_model.load_state_dict(torch.load(BEST_MODEL_SAVE_PATH, map_location=device))\n",
        "            if test_loader and len(test_dataset)>0: evaluate_model_and_report(final_model, test_loader, device, OUTPUT_DIR_FOR_PLOTS_AND_REPORTS)\n",
        "        except Exception as e: print(f\"Error loading best model: {e}\")\n",
        "    elif 'model' in locals() and model is not None:\n",
        "        print(f\"\\n--- Evaluation (Current CNN-ViT Model) ---\")\n",
        "        if test_loader and len(test_dataset)>0: evaluate_model_and_report(model, test_loader, device, OUTPUT_DIR_FOR_PLOTS_AND_REPORTS)\n",
        "\n",
        "    gc.collect();\n",
        "    if device.type == 'cuda': torch.cuda.empty_cache()\n",
        "    print(\"\\n--- Main CNN-ViT script execution finished ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS_rN2x-4kRv",
        "outputId": "1c8560fe-6519-4011-d58f-c7c9ff342377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted.\n",
            "Output directories on Drive ensured/created at /content/drive/MyDrive/final_balanced_data/cnn__vit_SL30_output\n",
            "--- Main CNN-ViT Training Script Execution Started ---\n",
            "--- Config: Device=cuda, SeqLen=30, Batch=8, Epochs=10, LR=4e-05, WD=0.0001, Dropout=0.5\n",
            "ViT Params: Embed=768, Heads=12, Layers=6, MLP_Dim=3072\n",
            "Data Dirs: Real='/content/local_data/real_frames', Fake='/content/local_data/fake_frames', Flow='/content/local_data/precomputed_flow_features'\n",
            "Freeze CNN: True\n",
            "\n",
            "--- Loading Frame Paths from local storage ---\n",
            "Frames: Real=175840, Fake=161107\n",
            "\n",
            "--- Grouping Frames by Video ---\n",
            "Groups: Real=2931, Fake=2687\n",
            "Total sequences created: 11119 (Real: 5846, Fake: 5273)\n",
            "\n",
            "--- Balancing & Splitting Data ---\n",
            "Sequences after balancing: 10546\n",
            "Split sizes: Train=8437, Val=1054, Test=1055\n",
            "DataLoaders ready.\n",
            "\n",
            "--- Initializing CNN-ViT Model ---\n",
            "INFO: Initializing ResNet18 CNN with ImageNet pretrained weights for CNN-ViT.\n",
            "INFO: Frozen ResNet18 layers in cnn_backbone up to child index 5.\n",
            "\n",
            "--- Starting CNN-ViT Model Training ---\n",
            "--- train_model (CNN-ViT): Starting training for 10 epochs ---\n",
            "Optimizer: AdamW, Group0_LR:4.0e-06(30 params), Group1_LR:4.0e-05(75 params), Group2_LR:8.0e-05(6 params), Weight Decay: 0.0001, Early Stop: 4\n",
            "Epoch 1/10, Train Loss: 0.3265, Train Acc: 0.8806, Val Loss: 0.2834, Val Acc: 0.9353, Duration: 2089.07s, LRs: ['4.0e-06', '4.0e-05', '8.0e-05']\n",
            "  Best model (val_loss) saved: 0.2834, Val Acc: 0.9353\n",
            "Epoch 2/10, Train Loss: 0.2441, Train Acc: 0.9377, Val Loss: 0.2784, Val Acc: 0.9087, Duration: 2104.74s, LRs: ['4.0e-06', '4.0e-05', '8.0e-05']\n",
            "  Best model (val_loss) saved: 0.2784, Val Acc: 0.9087\n",
            "Epoch 3/10, Train Loss: 0.1957, Train Acc: 0.9488, Val Loss: 0.0425, Val Acc: 0.9873, Duration: 2090.80s, LRs: ['4.0e-06', '4.0e-05', '8.0e-05']\n",
            "  Best model (val_loss) saved: 0.0425, Val Acc: 0.9873\n",
            "Epoch 4/10, Train Loss: 0.1620, Train Acc: 0.9651, Val Loss: 0.3245, Val Acc: 0.9514, Duration: 2065.58s, LRs: ['4.0e-06', '4.0e-05', '8.0e-05']\n",
            "Epoch 5/10, Train Loss: 0.1373, Train Acc: 0.9702, Val Loss: 0.0288, Val Acc: 0.9919, Duration: 2046.21s, LRs: ['4.0e-06', '4.0e-05', '8.0e-05']\n",
            "  Best model (val_loss) saved: 0.0288, Val Acc: 0.9919\n",
            "Epoch 6/10, Train Loss: 0.1436, Train Acc: 0.9695, Val Loss: 0.0535, Val Acc: 0.9884, Duration: 2068.38s, LRs: ['4.0e-06', '4.0e-05', '8.0e-05']\n",
            "Epoch 7/10, Train Loss: 0.1304, Train Acc: 0.9763, Val Loss: 0.2237, Val Acc: 0.9653, Duration: 2054.95s, LRs: ['4.0e-06', '4.0e-05', '8.0e-05']\n",
            "Epoch 8/10, Train Loss: 0.1148, Train Acc: 0.9770, Val Loss: 0.0224, Val Acc: 0.9919, Duration: 2066.92s, LRs: ['4.0e-06', '4.0e-05', '8.0e-05']\n",
            "  Best model (val_loss) saved: 0.0224, Val Acc: 0.9919\n",
            "Epoch 9/10, Train Loss: 0.1060, Train Acc: 0.9781, Val Loss: 0.4112, Val Acc: 0.9445, Duration: 2032.04s, LRs: ['4.0e-06', '4.0e-05', '8.0e-05']\n",
            "Epoch 10/10, Train Loss: 0.0873, Train Acc: 0.9828, Val Loss: 0.1287, Val Acc: 0.9827, Duration: 2066.30s, LRs: ['4.0e-06', '4.0e-05', '8.0e-05']\n",
            "Training finished. Total time: 20690.17s (344.84 min)\n",
            "Training history plot saved to /content/drive/MyDrive/final_balanced_data/cnn__vit_SL30_output/reports_and_plots_cnn_vit/training_history_cnn_vit.png\n",
            "\n",
            "--- Evaluation (Best CNN-ViT Model) ---\n",
            "INFO: Initializing ResNet18 CNN with ImageNet pretrained weights for CNN-ViT.\n",
            "\n",
            "--- Evaluation (CNN-ViT) ---\n",
            "Accuracy: 0.9942, Weighted F1: 0.9942\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       1.00      0.99      0.99       412\n",
            "        Fake       0.99      1.00      0.99       457\n",
            "\n",
            "    accuracy                           0.99       869\n",
            "   macro avg       0.99      0.99      0.99       869\n",
            "weighted avg       0.99      0.99      0.99       869\n",
            "\n",
            "CM:\n",
            "[[409   3]\n",
            " [  2 455]]\n",
            "Eval report saved to /content/drive/MyDrive/final_balanced_data/cnn__vit_SL30_output/reports_and_plots_cnn_vit/eval_report_cnn_vit.txt\n",
            "Confusion matrix plot saved to /content/drive/MyDrive/final_balanced_data/cnn__vit_SL30_output/reports_and_plots_cnn_vit/cm_cnn_vit.png\n",
            "\n",
            "--- Main CNN-ViT script execution finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === VIDEO-LEVEL INFERENCE ON CROSS-DATASET (CNN-ViT) ===\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import glob\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "from collections import defaultdict\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# --- Mount Drive ---\n",
        "# Assuming drive is already mounted. If not, uncomment the next line.\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- Config ---\n",
        "# <<< --- CHANGE THIS PATH to your saved CNN-ViT model --- >>>\n",
        "SAVED_MODEL_PATH = \"/content/drive/MyDrive/final_balanced_data/cnn__vit_SL30_output/best_model_cnn_vit_L6.pth\"\n",
        "\n",
        "# Data paths (can remain the same if using the same test set)\n",
        "FINAL_REAL_FRAMES_ZIP = \"/content/drive/MyDrive/Test_Dataset/Real/Frames_Real.zip\"\n",
        "FINAL_FAKE_FRAMES_ZIP = \"/content/drive/MyDrive/Test_Dataset/Fake/Frames_Fake.zip\"\n",
        "FINAL_REAL_FLOW_ZIP   = \"/content/drive/MyDrive/Test_Dataset/Real/Flow_Real.zip\"\n",
        "FINAL_FAKE_FLOW_ZIP   = \"/content/drive/MyDrive/Test_Dataset/Fake/Flow_Fake.zip\"\n",
        "\n",
        "# Local unzip dirs\n",
        "FRAMES_BASE_DIR = \"/content/Frames\"\n",
        "FLOW_BASE_DIR   = \"/content/Flow\"\n",
        "\n",
        "# Model Hyperparameters (must match the trained CNN-ViT model)\n",
        "VIT_EMBED_DIM = 768\n",
        "VIT_NUM_HEADS = 12\n",
        "VIT_NUM_LAYERS = 6\n",
        "VIT_MLP_DIM = 3072\n",
        "FLOW_DIM = 2\n",
        "NUM_CLASSES = 2\n",
        "DROPOUT_RATE = 0.5 # Match the rate used in your training script's classifier\n",
        "SEQ_LENGTH = 30\n",
        "INFERENCE_STEP_SIZE = SEQ_LENGTH // 2  # 50% overlap\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Helper: unzip (assuming it's already defined and run from the LSTM script) ---\n",
        "def unzip_to_dir(zip_path, extract_dir):\n",
        "    os.makedirs(extract_dir, exist_ok=True)\n",
        "    if not os.listdir(extract_dir):\n",
        "        print(f\"Extracting {zip_path}...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "            zf.extractall(extract_dir)\n",
        "        print(f\"✅ Extracted {zip_path} -> {extract_dir}\")\n",
        "    else:\n",
        "        print(f\"ℹ️ Using existing extracted folder: {extract_dir}\")\n",
        "\n",
        "# Unzip frames and flow if not already done\n",
        "unzip_to_dir(FINAL_REAL_FRAMES_ZIP, os.path.join(FRAMES_BASE_DIR, \"Real\"))\n",
        "unzip_to_dir(FINAL_FAKE_FRAMES_ZIP, os.path.join(FRAMES_BASE_DIR, \"Fake\"))\n",
        "unzip_to_dir(FINAL_REAL_FLOW_ZIP,   os.path.join(FLOW_BASE_DIR, \"Real\"))\n",
        "unzip_to_dir(FINAL_FAKE_FLOW_ZIP,   os.path.join(FLOW_BASE_DIR, \"Fake\"))\n",
        "\n",
        "# --- Image Transform (Identical to maintain consistency) ---\n",
        "IMG_TRANSFORM = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# --- Model Definition (CNN-ViT) ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=50):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class DeepfakeDetectionModel_CNNViT(nn.Module):\n",
        "    def __init__(self, cnn_output_dim_from_resnet=512, vit_embed_dim=VIT_EMBED_DIM,\n",
        "                 vit_num_heads=VIT_NUM_HEADS, vit_num_layers=VIT_NUM_LAYERS,\n",
        "                 vit_mlp_dim=VIT_MLP_DIM, sequence_length=SEQ_LENGTH,\n",
        "                 flow_dim=FLOW_DIM, num_classes=NUM_CLASSES, dropout_rate=DROPOUT_RATE,\n",
        "                 freeze_cnn_layers=False): # freeze_cnn_layers is irrelevant for inference\n",
        "        super(DeepfakeDetectionModel_CNNViT, self).__init__()\n",
        "        _resnet = models.resnet18(weights=None) # No need for pretrained weights when loading a state_dict\n",
        "        self.cnn_backbone = nn.Sequential(*list(_resnet.children())[:-1])\n",
        "        self.cnn_feature_proj = nn.Linear(cnn_output_dim_from_resnet, vit_embed_dim)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, vit_embed_dim))\n",
        "        self.pos_encoder = PositionalEncoding(vit_embed_dim, max_len=sequence_length + 1)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=vit_embed_dim, nhead=vit_num_heads, dim_feedforward=vit_mlp_dim,\n",
        "            dropout=dropout_rate, batch_first=True, activation='gelu'\n",
        "        )\n",
        "        self.vit_encoder = nn.TransformerEncoder(encoder_layer, num_layers=vit_num_layers)\n",
        "        self.flow_dim = flow_dim\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(vit_embed_dim + self.flow_dim),\n",
        "            nn.Linear(vit_embed_dim + self.flow_dim, vit_embed_dim // 2), nn.GELU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(vit_embed_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, frames, flow_features_sequence):\n",
        "        batch_size, seq_len, _, _, _ = frames.size()\n",
        "        cnn_features_list = [self.cnn_backbone(frames[:, t, :, :, :]).view(batch_size, -1) for t in range(seq_len)]\n",
        "        cnn_sequence_features = torch.stack(cnn_features_list, dim=1)\n",
        "        vit_input_features = self.cnn_feature_proj(cnn_sequence_features)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat((cls_tokens, vit_input_features), dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "        vit_encoded_output = self.vit_encoder(x)\n",
        "        vit_sequence_output = vit_encoded_output[:, 0]\n",
        "        if flow_features_sequence is not None and flow_features_sequence.ndim == 3:\n",
        "            aggregated_flow_features = torch.mean(flow_features_sequence, dim=1)\n",
        "        else:\n",
        "            aggregated_flow_features = torch.zeros(batch_size, self.flow_dim, device=frames.device)\n",
        "        combined_features = torch.cat((vit_sequence_output, aggregated_flow_features), dim=1)\n",
        "        return self.classifier(combined_features)\n",
        "\n",
        "# --- Load Model ---\n",
        "model = DeepfakeDetectionModel_CNNViT().to(DEVICE)\n",
        "model.load_state_dict(torch.load(SAVED_MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "print(\"✅ CNN-ViT Model loaded successfully.\")\n",
        "\n",
        "# --- Helper: Load sequence of frames (Identical) ---\n",
        "def load_frame_sequence(frame_paths):\n",
        "    imgs = [IMG_TRANSFORM(Image.open(p).convert(\"RGB\")) for p in frame_paths]\n",
        "    return torch.stack(imgs)\n",
        "\n",
        "# --- Collect sequences with overlapping (Identical) ---\n",
        "def collect_sequences(label):\n",
        "    frame_dirs = sorted(glob.glob(os.path.join(FRAMES_BASE_DIR, label, \"*\")))\n",
        "    sequences = defaultdict(list)\n",
        "    for vid_dir in frame_dirs:\n",
        "        vid_name = Path(vid_dir).stem\n",
        "        frame_paths = sorted(glob.glob(os.path.join(vid_dir, \"*.jpg\")))\n",
        "        for seq_idx in range(0, len(frame_paths) - SEQ_LENGTH + 1, INFERENCE_STEP_SIZE):\n",
        "            seq_frames = frame_paths[seq_idx: seq_idx + SEQ_LENGTH]\n",
        "            if len(seq_frames) == SEQ_LENGTH:\n",
        "                # Assuming flow naming convention is consistent\n",
        "                flow_name = f\"{label.lower()}_{vid_name}_seq{seq_idx//INFERENCE_STEP_SIZE}.pt\"\n",
        "                flow_path = os.path.join(FLOW_BASE_DIR, label, flow_name)\n",
        "                if os.path.exists(flow_path):\n",
        "                    sequences[vid_name].append((seq_frames, flow_path, 0 if label==\"Real\" else 1))\n",
        "    return sequences\n",
        "\n",
        "print(\"📂 Collecting sequences for CNN-ViT inference...\")\n",
        "real_sequences = collect_sequences(\"Real\")\n",
        "fake_sequences = collect_sequences(\"Fake\")\n",
        "\n",
        "# --- Hybrid decision rule (Identical) ---\n",
        "def hybrid_video_decision(seq_outputs, avg_threshold=0.45, seq_threshold=0.55):\n",
        "    probs_list = [torch.softmax(out, dim=1).squeeze().cpu().numpy() for out in seq_outputs]\n",
        "    fake_probs = [p[1] for p in probs_list]\n",
        "    avg_fake = np.mean(fake_probs)\n",
        "    any_fake = any(p >= seq_threshold for p in fake_probs)\n",
        "    if avg_fake >= avg_threshold or any_fake:\n",
        "        return 1  # Fake\n",
        "    return 0  # Real\n",
        "\n",
        "# --- Run inference (Identical logic) ---\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for vid_dict, label_name in [(real_sequences, \"Real\"), (fake_sequences, \"Fake\")]:\n",
        "        for vid_name, seq_list in vid_dict.items():\n",
        "            true_label = 0 if label_name == \"Real\" else 1\n",
        "            seq_outputs = []\n",
        "            for frame_paths, flow_path, _ in seq_list:\n",
        "                frames_tensor = load_frame_sequence(frame_paths).unsqueeze(0).to(DEVICE)\n",
        "                flow_tensor = torch.load(flow_path).unsqueeze(0).to(DEVICE)\n",
        "                outputs = model(frames_tensor, flow_tensor)\n",
        "                seq_outputs.append(outputs)\n",
        "            if seq_outputs:\n",
        "                # Using the same decision thresholds for a fair comparison\n",
        "                final_pred = hybrid_video_decision(seq_outputs, avg_threshold=0.4, seq_threshold=0.6)\n",
        "                y_true.append(true_label)\n",
        "                y_pred.append(final_pred)\n",
        "\n",
        "# --- Metrics (Identical) ---\n",
        "print(\"\\n--- FINAL EVALUATION RESULTS (CNN-ViT) ---\")\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "print(f\"Overall Accuracy: {acc:.4f} ({acc:.2%})\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Real\", \"Fake\"]))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Real\",\"Fake\"], yticklabels=[\"Real\",\"Fake\"])\n",
        "plt.title(\"Confusion Matrix - CNN-ViT Video-Level Cross Dataset\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "id": "dSg87XakpOqY",
        "outputId": "c210e35c-1771-42a3-8fbe-5e21b9af5775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Extracting /content/drive/MyDrive/Test_Dataset/Real/Frames_Real.zip...\n",
            "✅ Extracted /content/drive/MyDrive/Test_Dataset/Real/Frames_Real.zip -> /content/Frames/Real\n",
            "Extracting /content/drive/MyDrive/Test_Dataset/Fake/Frames_Fake.zip...\n",
            "✅ Extracted /content/drive/MyDrive/Test_Dataset/Fake/Frames_Fake.zip -> /content/Frames/Fake\n",
            "Extracting /content/drive/MyDrive/Test_Dataset/Real/Flow_Real.zip...\n",
            "✅ Extracted /content/drive/MyDrive/Test_Dataset/Real/Flow_Real.zip -> /content/Flow/Real\n",
            "Extracting /content/drive/MyDrive/Test_Dataset/Fake/Flow_Fake.zip...\n",
            "✅ Extracted /content/drive/MyDrive/Test_Dataset/Fake/Flow_Fake.zip -> /content/Flow/Fake\n",
            "✅ CNN-ViT Model loaded successfully.\n",
            "📂 Collecting sequences for CNN-ViT inference...\n",
            "\n",
            "--- FINAL EVALUATION RESULTS (CNN-ViT) ---\n",
            "Overall Accuracy: 0.9160 (91.60%)\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.87      0.98      0.92       250\n",
            "        Fake       0.98      0.85      0.91       250\n",
            "\n",
            "    accuracy                           0.92       500\n",
            "   macro avg       0.92      0.92      0.92       500\n",
            "weighted avg       0.92      0.92      0.92       500\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === SINGLE VIDEO UPLOAD + ON-THE-FLY INFERENCE (CNN-ViT) ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab import drive, files\n",
        "import math\n",
        "\n",
        "# --- Config ---\n",
        "# <<< --- CHANGE THIS PATH to your saved CNN-ViT model --- >>>\n",
        "SAVED_MODEL_PATH = \"/content/drive/MyDrive/final_balanced_data/cnn__vit_SL30_output/best_model_cnn_vit_L6.pth\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model Hyperparameters (must match the trained CNN-ViT model)\n",
        "SEQ_LENGTH = 30\n",
        "VIT_EMBED_DIM = 768\n",
        "VIT_NUM_HEADS = 12\n",
        "VIT_NUM_LAYERS = 6\n",
        "VIT_MLP_DIM = 3072\n",
        "FLOW_DIM = 2\n",
        "NUM_CLASSES = 2\n",
        "DROPOUT_RATE = 0.5 # Match the rate used in your training script's classifier\n",
        "OVERLAP = 15  # frames overlap between sequences\n",
        "\n",
        "IMG_TRANSFORM = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# --- Mount Drive ---\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- Model Definition (CNN-ViT) ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    # ... (keep this class definition as is)\n",
        "    def __init__(self, d_model, max_len=50):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class DeepfakeDetectionModel_CNNViT(nn.Module):\n",
        "    # ... (keep this class definition as is)\n",
        "    def __init__(self):\n",
        "        super(DeepfakeDetectionModel_CNNViT, self).__init__()\n",
        "        _resnet = models.resnet18(weights=None) # No pretrained weights needed for inference\n",
        "        self.cnn_backbone = nn.Sequential(*list(_resnet.children())[:-1])\n",
        "        self.cnn_feature_proj = nn.Linear(512, VIT_EMBED_DIM)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, VIT_EMBED_DIM))\n",
        "        self.pos_encoder = PositionalEncoding(VIT_EMBED_DIM, max_len=SEQ_LENGTH + 1)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=VIT_EMBED_DIM, nhead=VIT_NUM_HEADS, dim_feedforward=VIT_MLP_DIM,\n",
        "            dropout=DROPOUT_RATE, batch_first=True, activation='gelu'\n",
        "        )\n",
        "        self.vit_encoder = nn.TransformerEncoder(encoder_layer, num_layers=VIT_NUM_LAYERS)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(VIT_EMBED_DIM + FLOW_DIM),\n",
        "            nn.Linear(VIT_EMBED_DIM + FLOW_DIM, VIT_EMBED_DIM // 2), nn.GELU(),\n",
        "            nn.Dropout(DROPOUT_RATE),\n",
        "            nn.Linear(VIT_EMBED_DIM // 2, NUM_CLASSES)\n",
        "        )\n",
        "\n",
        "    def forward(self, frames, flow_features_sequence=None):\n",
        "        batch_size, seq_len, _, _, _ = frames.size()\n",
        "        cnn_features_list = [self.cnn_backbone(frames[:, t, :, :, :]).view(batch_size, -1) for t in range(seq_len)]\n",
        "        cnn_sequence_features = torch.stack(cnn_features_list, dim=1)\n",
        "        vit_input_features = self.cnn_feature_proj(cnn_sequence_features)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat((cls_tokens, vit_input_features), dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "        vit_encoded_output = self.vit_encoder(x)\n",
        "        vit_sequence_output = vit_encoded_output[:, 0]\n",
        "        if flow_features_sequence is not None:\n",
        "            aggregated_flow_features = torch.mean(flow_features_sequence, dim=1)\n",
        "        else:\n",
        "            aggregated_flow_features = torch.zeros(batch_size, FLOW_DIM, device=frames.device)\n",
        "        combined_features = torch.cat((vit_sequence_output, aggregated_flow_features), dim=1)\n",
        "        return self.classifier(combined_features)\n",
        "\n",
        "\n",
        "# --- Define All Helper Functions ---\n",
        "def compute_optical_flow(frames):\n",
        "    # ... (keep this function definition as is)\n",
        "    flows = [np.array([0.0, 0.0], dtype=np.float32)] # Pad for first frame\n",
        "    for i in range(len(frames) - 1):\n",
        "        prev_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
        "        next_gray = cv2.cvtColor(frames[i+1], cv2.COLOR_BGR2GRAY)\n",
        "        flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None,\n",
        "                                            0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "        mag = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)\n",
        "        flows.append(np.array([np.mean(mag), np.var(mag)], dtype=np.float32))\n",
        "    return torch.tensor(np.array(flows), dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "\n",
        "def frames_to_tensor(frames_seq):\n",
        "    # ... (keep this function definition as is)\n",
        "    imgs = [IMG_TRANSFORM(Image.fromarray(cv2.cvtColor(f, cv2.COLOR_BGR2RGB))) for f in frames_seq]\n",
        "    return torch.stack(imgs).unsqueeze(0)\n",
        "\n",
        "\n",
        "def predict_video(video_path, seq_length=SEQ_LENGTH, overlap=OVERLAP, max_sequences=5):\n",
        "    # ... (keep this function definition as is)\n",
        "    print(f\"Analyzing up to the first {max_sequences} sequences of the video...\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "    if not frames:\n",
        "        print(\"Error: Could not read any frames from the video.\")\n",
        "        return -1, []\n",
        "    if len(frames) < seq_length:\n",
        "        print(f\"Warning: Video is shorter than one sequence ({len(frames)} frames). Padding to {seq_length}.\")\n",
        "        frames.extend([frames[-1]] * (seq_length - len(frames)))\n",
        "    sequences = []\n",
        "    step = seq_length - overlap\n",
        "    for start in range(0, len(frames) - seq_length + 1, step):\n",
        "        sequences.append(frames[start : start + seq_length])\n",
        "        if len(sequences) >= max_sequences:\n",
        "            break\n",
        "    seq_preds = []\n",
        "    with torch.no_grad():\n",
        "        for i, seq in enumerate(sequences):\n",
        "            print(f\"  Processing sequence {i+1}/{len(sequences)}...\")\n",
        "            frames_tensor = frames_to_tensor(seq).to(DEVICE)\n",
        "            flow_tensor = compute_optical_flow(seq).to(DEVICE)\n",
        "            out = model(frames_tensor, flow_tensor)\n",
        "            probs = torch.softmax(out, dim=1).squeeze().cpu().numpy()\n",
        "            seq_preds.append(int(np.argmax(probs)))\n",
        "    if not seq_preds:\n",
        "        print(\"Warning: No sequences were processed (video might be too short).\")\n",
        "        return -1, []\n",
        "    final_pred = max(set(seq_preds), key=seq_preds.count)\n",
        "    return final_pred, seq_preds\n",
        "\n",
        "\n",
        "# --- MAIN EXECUTION LOGIC ---\n",
        "# Load the model AFTER defining the class\n",
        "model = DeepfakeDetectionModel_CNNViT().to(DEVICE)\n",
        "model.load_state_dict(torch.load(SAVED_MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "print(\"✅ CNN-ViT Model loaded successfully.\")\n",
        "\n",
        "# Now, ask for the upload\n",
        "print(\"\\nPlease upload a video file to analyze.\")\n",
        "uploaded = files.upload()\n",
        "video_file = list(uploaded.keys())[0]\n",
        "\n",
        "# Finally, run the prediction\n",
        "label, seqs = predict_video(video_file)\n",
        "if label != -1:\n",
        "    print(\"\\n--- PREDICTION ---\")\n",
        "    prediction_text = \"FAKE\" if label == 1 else \"REAL\"\n",
        "    print(f\"Predicted Class: {prediction_text} (raw label: {label})\")\n",
        "    print(f\"Sequence-level predictions (0=Real, 1=Fake): {seqs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "EqzvqGu-cZ3l",
        "outputId": "f10bb09b-dd48-4633-852e-027c1f03388d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ CNN-ViT Model loaded successfully.\n",
            "\n",
            "Please upload a video file to analyze.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8ba65b84-9097-457e-8ad1-e18e76285811\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8ba65b84-9097-457e-8ad1-e18e76285811\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving id20_0007.mp4 to id20_0007.mp4\n",
            "Analyzing up to the first 5 sequences of the video...\n",
            "  Processing sequence 1/5...\n",
            "  Processing sequence 2/5...\n",
            "  Processing sequence 3/5...\n",
            "  Processing sequence 4/5...\n",
            "  Processing sequence 5/5...\n",
            "\n",
            "--- PREDICTION ---\n",
            "Predicted Class: REAL (raw label: 0)\n",
            "Sequence-level predictions (0=Real, 1=Fake): [0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    }
  ]
}